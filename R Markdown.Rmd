---
title: "Untitled"
output:
  word_document: default
  html_document:
    df_print: paged
---


## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r message=FALSE,warning=FALSE}
library(NLP)
library(xml2)
library(proto)
library(RSQLite)
library(readr)
library(tm)
library(SnowballC)
library(RWeka)
library(sqldf)
library(dplyr)
library(data.table)
library(ggmap)
library(RODBC)
library(XML)
library(rvest)
library(xml2)
library(openxlsx)
library(RPostgreSQL)
library(stringr)
library(data.table)
library(stringr)
library(bitops)
library(RCurl)
library(rjson)
library(sp)
library(postGIStools)
library(rstudioapi)
```

```{r}
path1<-paste0(getwd(),"/Documents")
path2<-paste0(getwd(),"/Documents_backup")
path3<-paste0(getwd(),"/HTML_LINKS")
path4<-paste0(getwd(),"/Phantom/phantomjs-2.1.1-windows/bin")
```

```{r}
##CLEANING AND CONSTRUCTION OF TEXT MATRIX

## Step 1: To read the files - Folder:
setwd(path1)
files_names<-list.files(getwd()) 
doc.list<-lapply(files_names,read_file)
N.docs<-length(doc.list)
## Step 2: To construct Corpus of documents
my.docs<-VectorSource(c(doc.list))
my.docs$names<-sub(".txt","",files_names)
mydocs<-Corpus(my.docs)
## Step 3: To clean special characters
removeSpecialChars<-function(x) gsub("[^-a-zA-z0-9. ]","",x)
mydocs<-tm_map(mydocs,removeSpecialChars)
## Step 4: To clean stopwords()
mydocs<-tm_map(mydocs,removeWords,stopwords('en'))
## Step 5: To clean doble whitespaces between words
mydocs<-tm_map(mydocs,stripWhitespace)
## Step 6: To clean information before the word "Abstract"
removeAbstract<-function(x) gsub("(.*?)A(?i)bstract(?-i)","",x)
mydocs<-tm_map(mydocs,removeAbstract)
## Step 7: To clean information after the word "Conclusions"
removeConclusions<-function(x) sub("C(?i)onclusion(?-i)(.*)","",x)
mydocs<-tm_map(mydocs,removeConclusions)
## Step 8: To divide the text in four main parts: Abstract, Introduction, Materials and Methods and Results
## Step 8.1: Abstract
removeIntroduction_down<-function(x) sub("I(?i)ntroduction(?-i)(.*)","",x)
str1<-tm_map(mydocs,removeIntroduction_down)
## Step 8.2: Introduction
removeIntroduction_up<-function(x) sub("(.*?)I(?i)ntroduction(?-i)","",x)
str2_fs1.1<-tm_map(mydocs,removeIntroduction_up)
removeMethods_down<-function(x) sub("M(?i)ethods(?-i)(.*)","",x)
str2_fs1.2<-tm_map(str2_fs1.1,removeMethods_down)
removeMethods_down<-function(x) sub("M(?i)aterial(?-i)(.*)","",x)  
str2<-tm_map(str2_fs1.2,removeMethods_down)
## Step 8.3: Materials and Methods
removeMethods_up<-function(x) sub("(.*?)M(?i)aterial(?-i)","",x)
str3_fs1.1<-tm_map(mydocs,removeMethods_up)
removeMethods_up<-function(x) sub("(.*?)M(?i)ethods(?-i)","",x)
str3_fs1.2<-tm_map(str3_fs1.1,removeMethods_up)
removeResults_down<-function(x) sub("R(?i)esults(?-i)(.*)","",x)
str3<-tm_map(str3_fs1.2,removeResults_down)
## Step 8.4: Results
removeResults_up<-function(x) sub("(.*?)R(?i)esults(?-i)","",x)
str4<-tm_map(mydocs,removeResults_up)
## Step 9: To save information in data.frames for posterior construction of VCorpus
dataframe_docs<-data.frame(text=sapply(mydocs, as.character),stringsAsFactors = FALSE)
extract_str1<-data.frame(text=sapply(str1, as.character),stringsAsFactors = FALSE)
extract_str2<-data.frame(text=sapply(str2, as.character),stringsAsFactors = FALSE)
extract_str3<-data.frame(text=sapply(str3, as.character),stringsAsFactors = FALSE)
extract_str4<-data.frame(text=sapply(str4, as.character),stringsAsFactors = FALSE)
## Step 10: To create VCorpus for N-gram analysis
df<-data.frame(doc_id=seq(1:N.docs),text=dataframe_docs$text,stringsAsFactors = F)
VCorpus_docs<-VCorpus(DataframeSource(df))
df_str1<-data.frame(doc_id=seq(1:N.docs),text=extract_str1$text,stringsAsFactors = F)##Abstract
VCorpus_docs1<-VCorpus(DataframeSource(df_str1))
df_str2<-data.frame(doc_id=seq(1:N.docs),text=extract_str2$text,stringsAsFactors = F)##Introduction
VCorpus_docs2<-VCorpus(DataframeSource(df_str2))
df_str3<-data.frame(doc_id=seq(1:N.docs),text=extract_str3$text,stringsAsFactors = F)##Materials and methods
VCorpus_docs3<-VCorpus(DataframeSource(df_str3))
df_str4<-data.frame(doc_id=seq(1:N.docs),text=extract_str4$text,stringsAsFactors = F)##Results
VCorpus_docs4<-VCorpus(DataframeSource(df_str4))
```

```{r}
##PARAMETER_1 AND PARAMETER_2: COUNTRY AND COUNTRY_CODE
## Step 1: To read the dataset of countries in the world - Folder:
setwd(path2)
countries<-read.delim("GEODATASOURCE-COUNTRY.txt", header = TRUE, sep = "\t")
countries_names<-as.vector(countries[,4])
countries_codes<-as.vector(countries[,1])
## Step 2: To extract the number of countries in the dataset
l_countries<-length(countries_names)
country_code<- c(rep(NA,N.docs))
country<-c(rep(NA,N.docs))
## Step 3: To extract the country name
## Rules
## 1- SP's: Abstract, Materials and Methods
## 2- Tokanizer: min=1, max=2
  for (m in 1:N.docs){

         Tokenizer<-function(x) NGramTokenizer(x,Weka_control(min=1,max=2))
         head(NGramTokenizer(VCorpus_docs3[m],Weka_control(min=1,max=2)))
         dtm3<-TermDocumentMatrix(VCorpus_docs3[m],control=list(tokenize=Tokenizer,tolower=FALSE))
         bio3<-as.data.frame(t(as.matrix(dtm3)))
         keywords3<-colnames(bio3)
         kw_country3<-as.data.frame(keywords3)
         colnames(kw_country3)[1]<-"keywords_country"
         
         Tokenizer<-function(x) NGramTokenizer(x,Weka_control(min=1,max=2))
         head(NGramTokenizer(VCorpus_docs1[m],Weka_control(min=1,max=2)))
         dtm1<-TermDocumentMatrix(VCorpus_docs1[m],control=list(tokenize=Tokenizer,tolower=FALSE))
         bio1<-as.data.frame(t(as.matrix(dtm1)))
         keywords1<-colnames(bio1)
         kw_country1<-as.data.frame(keywords1)
         colnames(kw_country1)[1]<-"keywords_country"
         
         kw_country<-rbind(kw_country3,kw_country1)
         
         l_keywords<-length(keywords1)+length(keywords3)
         extract_country<-as.data.frame(lapply(kw_country, function(x) lapply(countries_names,function(y) grepl(y,x))))
         for (i in 1:l_keywords){
             for (j in 1:l_countries){
                 if (extract_country[i,j]=="TRUE"){
                     country[m]<-countries_names[j]
                     country_code[m]<-countries_codes[j]
                   }
               }
          }
      }

```

```{r message=FALSE,warning=FALSE}
##PARAMETER_3: MUNICIPALITY
## Step 1: To read the dataset of cities in the world - Folder:
setwd(path2)
cities<-fread("GEODATASOURCE-CITIES-FREE.txt", header = TRUE, sep = "\t")
cities_data<-as.data.frame(cities)
## Step 2: To extract the parameter City_name
## Rules
## 1- SP's: Materials and Methods
## 2- Tokanizer: min=1, max=4
MUNICIPALITY<-c(rep(NA,N.docs))
for (m in 1:N.docs){
  if (is.na(country_code[m])==FALSE){
    Tokenizer_city<-function(x) NGramTokenizer(x,Weka_control(min=1,max=4))
    head(NGramTokenizer(VCorpus_docs3[m],Weka_control(min=1,max=4)))
    dtm_city<-TermDocumentMatrix(VCorpus_docs3[m],control=list(tokenize=Tokenizer_city,tolower=FALSE))
    bio_city<-as.data.frame(t(as.matrix(dtm_city)))
    keywords_city<-colnames(bio_city)
    kw_city<-as.vector(keywords_city)
    l_keywords_city<-length(keywords_city)
    filtration_cities<-subset(cities_data,CC_FIPS==country_code[m])
    filtration_cities<-as.vector(filtration_cities[,2])
    l_cities<-length(filtration_cities)
    filtration_cities<-paste("^",filtration_cities,"$",sep="")
    extract_cities<-lapply(filtration_cities,function(x)grep(x,kw_city,value=TRUE))
    extract_cities<-extract_cities[lapply(extract_cities,length)>0]
    if ((NROW(extract_cities)==1)&(length(extract_cities[[1]])==1)){
      MUNICIPALITY[m]<-extract_cities[[1]][1]
    }
  }
}
```

```{r}
##PARAMETER_4 AND PARAMETER_5: COORDINATES (LATITUDE,LONGITUDE)
##Step 1.1: To extract coordinates using API from Gogle Maps based on the name of the city (MUNICIPALITY)
LONGITUDE<-c(rep(NA,N.docs))
LATITUDE<-c(rep(NA,N.docs))
getGeoData <- function(location){
  location <- gsub(' ','+',location)
  geo_data <- getURL(paste("https://maps.googleapis.com/maps/api/geocode/json?address=",location,"&key=AIzaSyD3r4Ckm47KQ0luZEPy7TuaH8TxMyRC0GQ", sep=""))
  raw_data_2 <- fromJSON(geo_data)
  return(raw_data_2)
}

for (m in 1:N.docs){
  
  if (is.na(MUNICIPALITY[m])=="FALSE"){
    
    coordinates<-unlist(getGeoData(MUNICIPALITY[m]))
    
    for (a in 1:length(coordinates)){
      if (names(coordinates)[a]=="results.geometry.location.lat"){
      LATITUDE[m]<-unname(coordinates[a])
      }
      if (names(coordinates)[a]=="results.geometry.location.lng"){
        LONGITUDE[m]<-unname(coordinates[a])
      }
    }
  }
##Step 1.2: To extract coordinates directly from the text
## Rules
## 1- SP's: Materials and Methods
## 2- Tokanizer: min=1, max=1
  
  if (is.na(MUNICIPALITY[m])=="TRUE"){
    Tokenizer<-function(x) NGramTokenizer(x,Weka_control(min=1,max=1))
    head(NGramTokenizer(VCorpus_docs3[m],Weka_control(min=1,max=1)))
    dtm_coordinates<-TermDocumentMatrix(VCorpus_docs3[m],control=list(tokenize=Tokenizer,tolower=FALSE))
    bio_coordinates<-as.data.frame(t(as.matrix(dtm_coordinates)))
    keywords_coordinates<-colnames(bio_coordinates)
    kw_coordinates<-as.data.frame(keywords_coordinates)  
    
    longitud_filter_E<-as.vector(filter(kw_coordinates, grepl(pattern="^[0-9]{5}E",kw_coordinates$keywords_coordinates))[1,])
    longitud_filter_W<-as.vector(filter(kw_coordinates, grepl(pattern="^[0-9]{5}W",kw_coordinates$keywords_coordinates))[1,])
    latitud_filter_N<-as.vector(filter(kw_coordinates, grepl(pattern="^[0-9]{5}N",kw_coordinates$keywords_coordinates))[1,])
    latitud_filter_S<-as.vector(filter(kw_coordinates, grepl(pattern="^[0-9]{5}S",kw_coordinates$keywords_coordinates))[1,])
    
    if (is.na(longitud_filter_E[1])=="FALSE"){
    
    lon_number<-sub("(^[^0-9]*)(\\d+)([^0-9].*)", "\\2", longitud_filter_E[1])
    grade_longitud<-as.numeric(sub('([0-9]{2}).*', '\\1', lon_number))
    minutes_longitud<-as.numeric(sub(".*(\\d+{2}).*$", "\\1", lon_number))
    LONGITUDE[m]<-grade_longitud+minutes_longitud/60
    }
    
    if (is.na(latitud_filter_N[1])=="FALSE"){
     lat_number<-sub("(^[^0-9]*)(\\d+)([^0-9].*)", "\\2", latitud_filter_N[1])
     grade_latitud<-as.numeric(sub('([0-9]{2}).*', '\\1', lat_number))
     minutes_latitud<-as.numeric(sub(".*(\\d+{2}).*$", "\\1", lat_number))
     LATITUDE[m]<-grade_latitud+minutes_latitud/60
    }
   
    if (is.na(longitud_filter_W[1])=="FALSE"){
      
      lon_number<-sub("(^[^0-9]*)(\\d+)([^0-9].*)", "\\2", longitud_filter_W[1])
      grade_longitud<-as.numeric(sub('([0-9]{2}).*', '\\1', lon_number))
      minutes_longitud<-as.numeric(sub(".*(\\d+{2}).*$", "\\1", lon_number))
      LONGITUDE[m]<-(grade_longitud+minutes_longitud/60)*(-1)
    } 
    
    if (is.na(latitud_filter_S[1])=="FALSE"){
      lat_number<-sub("(^[^0-9]*)(\\d+)([^0-9].*)", "\\2", latitud_filter_S[1])
      grade_latitud<-as.numeric(sub('([0-9]{2}).*', '\\1', lat_number))
      minutes_latitud<-as.numeric(sub(".*(\\d+{2}).*$", "\\1", lat_number))
      LATITUDE[m]<-(grade_latitud+minutes_latitud/60)*-1
    }
  }
}

```

```{r}
##PARAMETER_6: TYPE OF WETLAND
##Step 1: To define matrix of keywords
matrix_keyword<-c("^VSSF$",
                  "^HSSF$",
                  "^VFCW$",
                  "^HFCW$",
                  "^(?i)vertical(?-i)$",
                  "^(?i)surface flow(?-i)$",
                  "^(?i)subsurface flow(?-i)$")

matrix_keyword_nregex<-c("VSSF",
                         "HSSF",
                         "VFCW",
                         "HFCW",
                         "vertical",
                         "surface flow",
                         "subsurface flow")

##Step 2: To define cluster of expressions in matrix of keywords
cluster<-c("VF",
           "HSSF",
           "VF",
           "HF",
           "VF",
           "FWS",
           "HSSF")
##Step 3: To extract parameter
## Rules
## 1- SP's: Abstract, Materials and Methods, Results
## 2- Tokanizer: min=1, max=2

n.words<-length(matrix_keyword)
TYPE_WETLANDS<-vector(mode="character", length=N.docs)
variable_cluster<-vector(mode="character", length=10)

for (m in 1:N.docs){
  Tokanizer_typewetlands3<-function(x) NGramTokenizer(x,Weka_control(min=1,max=2))
  head(NGramTokenizer(VCorpus_docs3[m],Weka_control(min=1,max=2)))
  dtm_typewetlands3<-TermDocumentMatrix(VCorpus_docs3[m],control = list(tokenize=Tokanizer_typewetlands3,tolower=FALSE))
  bio_typewetlands3<-as.data.frame(t(as.matrix(dtm_typewetlands3)))
  keywords_typewetlands3<-colnames(bio_typewetlands3)
  kw_typewetlands3<-as.data.frame(keywords_typewetlands3)
  colnames(kw_typewetlands3)[1]<-"keywords_typewetlands"
  
  Tokanizer_typewetlands4<-function(x) NGramTokenizer(x,Weka_control(min=1,max=2))
  head(NGramTokenizer(VCorpus_docs4[m],Weka_control(min=1,max=2)))
  dtm_typewetlands4<-TermDocumentMatrix(VCorpus_docs4[m],control = list(tokenize=Tokanizer_typewetlands4,tolower=FALSE))
  bio_typewetlands4<-as.data.frame(t(as.matrix(dtm_typewetlands4)))
  keywords_typewetlands4<-colnames(bio_typewetlands4)
  kw_typewetlands4<-as.data.frame(keywords_typewetlands4)
  colnames(kw_typewetlands4)[1]<-"keywords_typewetlands"
  
  Tokanizer_typewetlands1<-function(x) NGramTokenizer(x,Weka_control(min=1,max=2))
  head(NGramTokenizer(VCorpus_docs1[m],Weka_control(min=1,max=2)))
  dtm_typewetlands1<-TermDocumentMatrix(VCorpus_docs1[m],control = list(tokenize=Tokanizer_typewetlands4,tolower=FALSE))
  bio_typewetlands1<-as.data.frame(t(as.matrix(dtm_typewetlands1)))
  keywords_typewetlands1<-colnames(bio_typewetlands1)
  kw_typewetlands1<-as.data.frame(keywords_typewetlands1)
  colnames(kw_typewetlands1)[1]<-"keywords_typewetlands"
  
  kw_typewetlands<-rbind(kw_typewetlands3,kw_typewetlands4,kw_typewetlands1)
  c<-0
  
    for(i in 1:n.words){
    length_filtered_terms_type<-nrow(filter(kw_typewetlands, grepl(pattern=matrix_keyword[i],kw_typewetlands$keywords_typewetlands)))
    if((length_filtered_terms_type>=1)&(TYPE_WETLANDS[m]!=""))
    {
      con<-0
      for (l in 1:c){
      if (variable_cluster[l]==cluster[i]){
      con<-1
      }
      }
      if (con==0){
        c<-c+1
        variable_cluster[c]<-cluster[i]
        TYPE_WETLANDS[m]<-paste0(TYPE_WETLANDS[m],"-",cluster[i])
      }
    }
    if((length_filtered_terms_type>=1)&(TYPE_WETLANDS[m]==""))
    {
      c<-c+1
      variable_cluster[c]<-cluster[i]
      TYPE_WETLANDS[m]<-cluster[i]
    }
  }
}
```

```{r}
##PARAMETER_6: AREA
##Step 1: To extract parameter
## Rules
## 1- SP's: Abstract, Materials and Methods
## 2- Tokanizer: min=2, max=2

AREA<-vector(mode="character", length=N.docs)
NLPBigramTokenizer <- function(x) {
  unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}

for (m in 1:N.docs){
  dtm_area<-TermDocumentMatrix(VCorpus_docs1[m],control = list(tokenize=NLPBigramTokenizer))
  bio_area<-as.data.frame(t(as.matrix(dtm_area)))
  keywords_area<-colnames(bio_area)
  kw_area<-as.data.frame(keywords_area)
  filtered_terms_area1 <- filter(kw_area, grepl(pattern="[0-9]m2.$",kw_area$keywords_area))
  filtered_terms_area2 <- filter(kw_area, grepl(pattern="[0-9] m2.$",kw_area$keywords_area))
  filtered_terms_area3 <- filter(kw_area, grepl(pattern="[[0-9]m2$",kw_area$keywords_area))
  filtered_terms_area4 <- filter(kw_area, grepl(pattern="[0-9] m2$",kw_area$keywords_area))
  filtered_terms_area<-rbind(filtered_terms_area1,filtered_terms_area2,filtered_terms_area3,filtered_terms_area4)
  filtered_terms_area<-as.vector(filtered_terms_area$keywords_area)

  if (length(filtered_terms_area)>0){
    for(i in 1:length(filtered_terms_area)){
      if(AREA[m]!="")
      {
        filtered_terms_area[i]<-sub("m(.*)","",gsub(gsub("[0-9].*","",filtered_terms_area[i]),"",filtered_terms_area[i]))
        filtered_terms_area[i]<-gsub("[[:space:]]+","",filtered_terms_area[i])
        AREA[m]<-paste0(AREA[m],"-",filtered_terms_area[i])
      }
      if(AREA[m]=="")
      {
        filtered_terms_area[i]<-sub("m(.*)","",gsub(gsub("[0-9].*","",filtered_terms_area[i]),"",filtered_terms_area[i]))
        filtered_terms_area[i]<-gsub("[[:space:]]+","",filtered_terms_area[i])
        AREA[m]<-filtered_terms_area[i]
      }
    }  
  }
  
  if (length(filtered_terms_area)==0){

    dtm_area<-TermDocumentMatrix(VCorpus_docs3[m],control = list(tokenize=NLPBigramTokenizer))
    bio_area<-as.data.frame(t(as.matrix(dtm_area)))
    keywords_area<-colnames(bio_area)
    kw_area<-as.data.frame(keywords_area)
    filtered_terms_area1 <- filter(kw_area, grepl(pattern="[0-9]m2.$",kw_area$keywords_area))
    filtered_terms_area2 <- filter(kw_area, grepl(pattern="[0-9] m2.$",kw_area$keywords_area))
    filtered_terms_area3 <- filter(kw_area, grepl(pattern="[[0-9]m2$",kw_area$keywords_area))
    filtered_terms_area4 <- filter(kw_area, grepl(pattern="[0-9] m2$",kw_area$keywords_area))
    filtered_terms_area<-rbind(filtered_terms_area1,filtered_terms_area2,filtered_terms_area3,filtered_terms_area4)
    filtered_terms_area<-as.vector(filtered_terms_area$keywords_area)
    
    if (length(filtered_terms_area)!=0){

      for(i in 1:length(filtered_terms_area)){
        if(AREA[m]!="")
        {
          filtered_terms_area[i]<-sub("m(.*)","",gsub(gsub("[0-9].*","",filtered_terms_area[i]),"",filtered_terms_area[i]))
          filtered_terms_area[i]<-gsub("[[:space:]]+","",filtered_terms_area[i])
          AREA[m]<-paste0(AREA[m],"-",filtered_terms_area[i])
        }
        if(AREA[m]=="")
        {
          filtered_terms_area[i]<-sub("m(.*)","",gsub(gsub("[0-9].*","",filtered_terms_area[i]),"",filtered_terms_area[i]))
          filtered_terms_area[i]<-gsub("[[:space:]]+","",filtered_terms_area[i])
          AREA[m]<-filtered_terms_area[i]
        }
      }     
    }
  }
}

```

```{r}
##PARAMETER_7: TYPE OF WASTEWATER
##Step 1: To define matrix of keywords
matrix_keyword<-c("^(?i)domestic(?-i)$",
                  "^(?i)industrial(?-i)$",
                  "^(?i)urban runoff(?-i)$",
                  "^(?i)food processing industry(?-i)$",
                  "^(?i)agriculture(?-i)$",
                  "^(?i)agricultural(?-i)$",
                  "^(?i)eutrophic lake water(?-i)$",
                  "^(?i)Dariy milking parlor(?-i)$",
                  "^(?i)Potato starch processing(?-i)$",
                  "^(?i)swine urine(?-i)$")

matrix_keyword_nregex<-c("domestic",
                         "industrial",
                         "urban runoff",
                         "food processing industry",
                         "agriculture",
                         "agricultural",
                         "eutrophic lake water",
                         "Dariy milking parlor",
                         "Potato starch processing",
                         "swine urine")

##Step 2: To define cluster of expressions in matrix of keywords
cluster<-c("MUNICIPAL",
           "INDUSTRIAL",
           "STORMWATER",
           "INDUSTRIAL",
           "AGRICULTURAL",
           "AGRICULTURAL",
           "INDUSTRIAL",
           "INDUSTRIAL",
           "INDUSTRIAL")
##Step 3: To extract parameter
## Rules
## 1- SP's: Materials and Methods
## 2- Tokanizer: min=1, max=3

variable_cluster<-vector(mode="character", length=10)
n.words<-length(matrix_keyword)
TYPE_WASTEWATER<-c(rep(NA,N.docs))

for (m in 1:N.docs){
  Tokanizer_typewastewater<-function(x) NGramTokenizer(x,Weka_control(min=1,max=3))
  head(NGramTokenizer(VCorpus_docs3[m],Weka_control(min=1,max=3)))
  dtm_typewastewater<-TermDocumentMatrix(VCorpus_docs3[m],control = list(tokenize=Tokanizer_typewastewater,tolower=FALSE))
  bio_typewastewater<-as.data.frame(t(as.matrix(dtm_typewastewater)))
  keywords_typewastewater<-colnames(bio_typewastewater)
  kw_typewastewater<-as.data.frame(keywords_typewastewater)
  c<-0
  
  for(i in 1:n.words){
    length_filtered_terms_type<-nrow(filter(kw_typewastewater, grepl(pattern=paste0("(?i)",matrix_keyword[i],"(?-i)"),kw_typewastewater$keywords_typewastewater)))
    if((length_filtered_terms_type>=1)&(is.na(TYPE_WASTEWATER[m])=="FALSE"))
    {
      con<-0
        for (l in 1:c){
          if (variable_cluster[l]==cluster[i]){
            con<-1
          }
        }
   
      if (con==0){
        c<-c+1
        variable_cluster[c]<-cluster[i]
        TYPE_WASTEWATER[m]<-paste0(TYPE_WASTEWATER[m],"-",cluster[i])
      }
      
    }
    if((length_filtered_terms_type>=1)&(is.na(TYPE_WASTEWATER[m])=="TRUE"))
    {
      c<-c+1
      variable_cluster[c]<-cluster[i]
      TYPE_WASTEWATER[m]<-cluster[i]
    }
  }
}
```

```{r}
##PARAMETER_8: TYPE OF PLANTS
##Step 1: To set the folder:
setwd(path2)
##Step 2: To read matrix of keywords
matrix<-as.data.frame(read.xlsx("Type_plants.xlsx"))
matrix_keywords<-as.vector(matrix[,1])
##Step 3: To read cluster of expressions in matrix of keywords
variable_cluster<-vector(mode="character", length=10)
cluster<-as.vector(matrix[,2])
##Step 4: To extract parameter
## Rules
## 1- SP's: Materials and Methods
## 2- Tokanizer: min=1, max=4

n.words<-length(matrix_keywords)
TYPE_PLANT<-vector(mode="character", length=N.docs)

for (m in 1:N.docs){
  Tokanizer_typeplant<-function(x) NGramTokenizer(x,Weka_control(min=1,max=4))
  head(NGramTokenizer(VCorpus_docs3[m],Weka_control(min=1,max=4)))
  dtm_typeplant<-TermDocumentMatrix(VCorpus_docs3[m],control = list(tokenize=Tokanizer_typeplant))
  bio_typeplant<-as.data.frame(t(as.matrix(dtm_typeplant)))
  keywords_typeplant<-colnames(bio_typeplant)
  kw_typeplant<-as.data.frame(keywords_typeplant)
  c<-0
  
  for(i in 1:n.words){
    length_filtered_terms_type<-nrow(filter(kw_typeplant, grepl(pattern=paste0("(?i)",matrix_keywords[i],"(?-i)"),kw_typeplant$keywords_typeplant)))
    if((length_filtered_terms_type>=1)&(TYPE_PLANT[m]!=""))
    {
      con<-0
      for (l in 1:c){
        if (variable_cluster[l]==cluster[i]){
          con<-1
        }
      }
      if (con==0){
        c<-c+1
        variable_cluster[c]<-cluster[i]
        TYPE_PLANT[m]<-paste0(TYPE_PLANT[m],";",cluster[i])
      }
    }
    if((length_filtered_terms_type>=1)&(TYPE_PLANT[m]==""))
    {
      c<-c+1
      variable_cluster[c]<-cluster[i]
      TYPE_PLANT[m]<-cluster[i]
    }
  }
}

```

```{r}
##PARAMETER_9: BOD5 INFLUENT, BOD5 EFLUENT, JOURNAL NAME, PUBLISHER, YEAR OF PUBLICATION, TITLE
##The following approach works for pre-reviwed articles, which information is available in HTML version e.g. ELSEVIER
## Step 1: To read the dataset of links - Folder:
setwd(path3)
HTML_links<-read.xlsx("HTML_links.xlsx")
HTML_names<-as.vector(HTML_links[,1])
HTML_url<-as.vector(HTML_links[,2])
## Step 2: To set PHANTOM folder:
setwd(path4)
## Step 3: To extract the parameters

BOD_inlist<-vector(mode="list",length=N.docs)
BOD_outlist<-vector(mode="list",length=N.docs)
BOD_removallist<-vector(mode="list",length=N.docs)
CITATION<-c(rep(NA,N.docs))
URL<-c(rep(NA,N.docs))
JOURN_NAME<-c(rep(NA,N.docs))
PUBLISHER<-c(rep(NA,N.docs))
LIT_TYPE<-c(rep(NA,N.docs))
YEAR<-c(rep(NA,N.docs))
TITLE<-vector(mode="character", length=N.docs)

for (m in 1:N.docs){
  
  LIT_TYPE[m]<-"PRE-REVIEW ARTICLE"
  
  for (j in 1:nrow(HTML_links)){
    if (my.docs$names[m]==HTML_names[j]){
      url<-HTML_url[j]
      URL[m]<-url
      name<-paste0("doc",m,".js")
      writeLines(sprintf("var page = require('webpage').create();
                         page.open('%s', function () {
                         console.log(page.content); //page source
                         phantom.exit();
                         });", url), con=name)
      name_phantom<-paste0("phantomjs doc",m,".js")
      name_html<-paste0("doc",m,".html")
      write(readLines(pipe(name_phantom, "r")), name_html)
      
      journal_elsevier<-grepl("sciencedirect",url,ignore.case=FALSE)##Journal: ELSEVIERR
      journal_IWA<-grepl("iwaponline",url,ignore.case=FALSE)##Journal:IWA
      journal_researchgate<-grepl("researchgate",url,ignore.case=FALSE)##Journal:Researchgate
      journal_ncbi<-grepl("ncbi",url,ignore.case=FALSE)##Journal:NCBI
      
      page_html<- read_html(name_html)
      table<-page_html %>% html_nodes("table") %>% html_table(fill=TRUE)
      head_lines<-page_html %>% html_nodes("span")%>%html_text()
      
      if (journal_elsevier=="TRUE"){
        title_article<-page_html %>% html_nodes("span.title-text")%>%html_text()
        TITLE[m]<-title_article
        surname<-page_html %>% html_nodes("span.text.surname")%>%html_text()
        name<-page_html %>% html_nodes("span.text.given-name")%>%html_text()
        doi<-page_html %>% html_nodes("a.doi")%>%html_text()
        doi<-sub("(.*?)doi.org","doi:",doi)
        journal_name<-page_html %>% html_nodes("h2#publication-title.publication-title")%>%html_text()
        publication_year<-as.data.frame(page_html %>% html_nodes("div.text-xs")%>%html_text())
        colnames(publication_year)[1]<-"YEAR"
        publication_year<-as.vector(filter(publication_year,grepl(pattern="(?i)volume(?-i)",publication_year$YEAR))[,1])
        name<-sub("Ã¤","ä",name)
        name<-sub("Ãµ","õ",name)
        name<-sub("Ã\u009c","Ü",name)
        surname<-sub("Ã\u0096Ã¶","Öö",surname)
        
        for (z in 1:length(surname)){
          
          if (z==1){
            authors<-paste0(surname[1]," ",name[1],",")
          }
          
          if ((z!=1)&(z!=length(surname))){
            authors<-paste0(authors,name[z]," ",surname[z],",")
          }
          
          if(z==length(surname)){
            
            authors<-paste0(authors,"and ",name[z]," ",surname[z],".")
            
          }
        }
        PUBLISHER[m]<-authors
        publication_year[1]<-sub("â\u0080\u0093","-",publication_year[1])
        part1<-sub("(.*?)[,]","",publication_year[1])
        part2<-sub("(.*?)[,] ","",part1)
        part3<-sub("[,](.*)","",part2)
        YEAR[m]<-str_sub(part3, start= -4)
        p<-as.numeric(YEAR[m])
        
        if ((is.na(p)=="TRUE")|(p<0)){
        part1<-sub("(.*?)[,]","",publication_year[1])
        part2<-sub("[,](.*)","",part1)
        YEAR[m]<-str_sub(part2, start= -4)
        }
        
        JOURN_NAME[m]<-journal_name
        CITATION[m]<-paste0(authors,'"',title_article,'"',".",journal_name," ",publication_year[1],".",doi) 
      }
      
      if (journal_IWA=="TRUE"){
        
        title_article<-page_html %>% html_nodes("h1#page-title.highwire-cite-title")%>%html_text()
        TITLE[m]<-title_article
        journal_name<-page_html %>% html_nodes("div.region-inner.region-branding-inner")%>%html_text()
        JOURN_NAME[m]<-journal_name
        name<-page_html %>% html_nodes("span.highwire-citation-authors")%>%html_text()
        PUBLISHER[m]<-name[1]
        doi_publicationyear<-page_html %>% html_nodes("div.highwire-cite-metadata")%>%html_text()
        CITATION[m]<-paste0(name[1],".",'"',title_article,'"',".Water Sci Technol.",doi_publicationyear[1])
        
      }
      
      if (journal_ncbi=="TRUE"){
        
        journal_name<-page_html %>% html_nodes("h1")%>%html_text()
        TITLE[m]<-journal_name[2]
        name<-page_html %>% html_nodes("div.auths")%>%html_text()
        PUBLISHER[m]<-name
        publication_year<-page_html %>% html_nodes("div.cit")%>%html_text()
        JOURN_NAME[m]<-sub("[.](.*)","",publication_year[1])
        part1<-sub("(.*?)[.] ","",publication_year[1])
        part2<-sub("[;](.*)","",part1)
        YEAR[m]<-gsub("[a-zA-z ]","",part2)
        PMID<-page_html %>% html_nodes("dd")%>%html_text()
        CITATION[m]<-paste0(name,'"',journal_name[2],'"',publication_year,"PMID: ",PMID[2])
        
      }
      
      head_tables<-vector(mode="character", length=20)
      a<-0
      
      for (i in 1:length(head_lines)){
        
        regex_head<-grepl("Table",head_lines[i],ignore.case=FALSE)
        
        if (regex_head=="TRUE"){
          a<-a+1
          head_tables[a]<-head_lines[i]
        }
      }
      
      head_tables<-head_tables[head_tables!=""]
      head_tables_clean<-vector(mode="character", length=20)
      c<-0
      for (i in 1:length(head_tables)){
        number<-length(unlist(str_extract_all(head_tables[i], "\\w+")))
        if (number>2){
          c<-c+1
          head_tables_clean[c]<-head_tables[i]
        }
      }
      head_tables_clean<-head_tables_clean[head_tables_clean!=""]
      
      BOD_IN<-vector(mode="integer", length=20)
      BOD_OUT<-vector(mode="integer", length=20)
      BOD_REMOVAL<-vector(mode="integer", length=20)
      sec_in<-0
      sec_out<-0
      sec_removal<-0
      
      if(length(table)>0){
        for(b in 1:length(table)){
          
          con5<-0
          con6<-0
          con2<-0
          con3<-0
          con4<-0
          
          regex_influent<-grepl("(?i)influent(?-i)",head_tables_clean[b],ignore.case=FALSE)|grepl("(?i)inflow(?-i)",head_tables_clean[b],ignore.case=FALSE)
          if (regex_influent==TRUE){
            con2<-1
          }
          
          regex_efluent<-grepl("(?i)efluent(?-i)",head_tables_clean[b],ignore.case=FALSE)|grepl("(?i)outflow(?-i)",head_tables_clean[b],ignore.case=FALSE)
          if (regex_efluent==TRUE){
            con3<-1
          }
          
          regex_removal<-grepl("(?i)removal(?i)",head_tables_clean[b],ignore.case=FALSE)
          if (regex_removal==TRUE){
            con4<-1
          }
          
          con<-0
          row<-nrow(table[[b]])
          col<-ncol(table[[b]])
          
          for(i in 1:row){
            for (j in 1:col){
              table[[b]][i,j]<-sub("(?i)Â(.*)(?-i)","",table[[b]][i,j])
              regex<-grepl("BOD",table[[b]][i,j],ignore.case=FALSE)|grepl("BOD",colnames(table[[b]])[j],ignore.case=FALSE)
              if (regex=="TRUE"){
                con<-con+1
              }
            }
          }
          
          table_numeric<-data.frame(table[[b]],stringsAsFactors=FALSE)
          table_numeric<- as.data.frame(sapply(table_numeric, as.numeric))
          
          elimination_colum<-vector(mode="integer", length=10)
          elimination_row<-vector(mode="integer", length=10)
          
          if (con>=1){
            
            for (j in 1:col){
              
              regex_deviation<-grepl("(?i)deviation(?-i)",table[[b]][1,j],ignore.case=FALSE)
              regex_deviation_names<-grepl("(?i)deviation(?-i)",colnames(table[[b]][j]),ignore.case=FALSE)
              
              if((regex_deviation==TRUE)|(regex_deviation_names==TRUE)){
                con5<-con5+1
                elimination_colum[con5]<-j
              }
            }
            
            for (i in 1:row){
              
              regex_deviation<-grepl("(?i)deviation(?-i)",table[[b]][i,1],ignore.case=FALSE)
              
              if(regex_deviation==TRUE){
                con6<-con6+1
                elimination_row[con6]<-i
              }
            }
            
            elimination_colum<-elimination_colum[elimination_colum!=0]  
            elimination_row<-elimination_row[elimination_row!=0]
            
            if ((length(elimination_colum)!=0)&(length(elimination_row)!=0)){
              table[[b]]<-table[[b]][-elimination_row,-elimination_colum]
            }
            if ((length(elimination_colum)==0)&(length(elimination_row)!=0)){
              table[[b]]<-table[[b]][-elimination_row,]
            }
            if ((length(elimination_colum)!=0)&(length(elimination_row)==0)){
              table[[b]]<-table[[b]][,-elimination_colum]
            }
          }
          
          row<-nrow(table[[b]])
          col<-ncol(table[[b]])
          
          if ((con2>=1)&(con>=1)){
            
            for (i in 1:2){
              for (j in 1:col){
                
                regex_in<-grepl("BOD",table[[b]][i,j],ignore.case=FALSE)
                
                k<-i+1
                if(k<=row){
                  if ((regex_in=="TRUE")&(is.numeric(table_numeric[k,j])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
                    p<-i+1
                    for (u in p:row){
                      sec_in<-sec_in+1
                      BOD_IN[sec_in]<-table[[b]][u,j]
                    } 
                  }
                }
              }
            }
            
            if (BOD_IN[1]==0){
              
              for (j in 1:col){
                
                regex_in<-grepl("BOD",colnames(table[[b]])[j],ignore.case=FALSE)&grepl("(?i)in(?-i)",table[[b]][1,j],ignore.case=FALSE)
                
                if (regex_in=="TRUE"){
                  
                  for (i in 2:row){
                    
                    sec_in<-sec_in+1
                    BOD_IN[sec_in]<-table[[b]][i,j]
                    
                  }
                }
              }
            }
            
            for (i in 1:row) {
              
              regex_in<-grepl("BOD",table[[b]][i,1],ignore.case=FALSE)
              
              if ((regex_in=="TRUE")&(is.numeric(table_numeric[i,2])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
                for (u in 2:col){
                  sec_in<-sec_in+1
                  BOD_IN[sec_in]<-table[[b]][i,u]
                } 
              }  
            }
            BOD_IN<-BOD_IN[BOD_IN!=0]
            BOD_inlist[[m]]<-BOD_IN
          } 
          
          
          if ((con3>=1)&(con>=1)){
            
            for (i in 1:2){
              for (j in 1:col){
                
                regex_out<-grepl("BOD",table[[b]][i,j],ignore.case=FALSE)
                k<-i+1
                if(k<=row){
                  if ((regex_out=="TRUE")&(is.numeric(table_numeric[k,j])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
                    p<-i+1
                    for (u in p:row){
                      sec_out<-sec_out+1
                      BOD_OUT[sec_out]<-table[[b]][u,j]
                    } 
                  }
                }
              }
            }
            
            if (BOD_OUT[1]==0){
              
              for (j in 1:col){
                
                regex_out<-grepl("BOD",colnames(table[[b]])[j],ignore.case=FALSE)&grepl("(?i)out(?-i)",table[[b]][1,j],ignore.case=FALSE)
                
                if (regex_out=="TRUE"){
                  
                  for (i in 2:row){
                    
                    sec_out<-sec_out+1
                    BOD_OUT[sec_out]<-table[[b]][i,j]
                    
                  }
                }
              }
            }
            
            for (i in 1:row) {
              
              regex_out<-grepl("BOD",table[[b]][i,1],ignore.case=FALSE)
              
              if ((regex_out=="TRUE")&(is.numeric(table_numeric[i,2])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
                for (u in 2:col){
                  sec_out<-sec_out+1
                  BOD_OUT[sec_out]<-table[[b]][i,u]
                } 
              }  
            }
            BOD_OUT<-BOD_OUT[BOD_OUT!=0]
            BOD_outlist[[m]]<-BOD_OUT
          }   
          
          if ((con4>=1)&(con>=1)){
            
            
            sec<-0
            for (i in 1:2){
              for (j in 1:col){
                
                regex_removal<-grepl("BOD",table[[b]][i,j],ignore.case=FALSE)
                k<-i+1
                if(k<=row){
                  if ((regex_removal=="TRUE")&(is.numeric(table_numeric[k,j])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
                    p<-i+1
                    for (u in p:row){
                      sec_removal<-sec_removal+1
                      BOD_REMOVAL[sec_removal]<-table[[b]][u,j]
                    } 
                  }
                }
              }
            }
            
            for (i in 1:row) {
              
              regex_removal<-grepl("BOD",table[[b]][i,1],ignore.case=FALSE)
              
              if ((regex_removal=="TRUE")&(is.numeric(table_numeric[i,2])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
                for (u in 2:col){
                  sec_removal<-sec_removal+1
                  BOD_REMOVAL[sec_removal]<-table[[b]][i,u]
                } 
              }  
            }
            BOD_REMOVAL<-BOD_REMOVAL[BOD_REMOVAL!=0]
            BOD_removallist[[m]]<-BOD_REMOVAL
          }
        }
      }
    }
}
  }

for (i in 1:N.docs){

  if (length(BOD_inlist[[i]])==0){
  BOD_inlist[[i]]<-c(NA)
  }
  
  if (length(BOD_outlist[[i]])==0){
    BOD_outlist[[i]]<-c(NA)
  }
  
  if (length(BOD_removallist[[i]])==0){
    BOD_removallist[[i]]<-c(NA)
  }
  
  BOD_removallist[[i]]<-as.numeric(BOD_removallist[[i]])
  BOD_outlist[[i]]<-as.numeric(BOD_outlist[[i]])
  BOD_inlist[[i]]<-as.numeric(BOD_inlist[[i]])
}
```

```{r}
##CREATION MATRIX OF PARAMETER: The parameters extracted from the text are integrated in a data.frame
##Step 1: To open the database in PostgreSQL

pw <- {
  "mauricio"
}##pw=password for accesing to the database

drv <- dbDriver("PostgreSQL")
con <- dbConnect(drv, dbname = "postgres",
                 host = "localhost", port = 5432,
                 user = "openpg", password = pw)

##Step 2: To read the tables in the PostgreSQL database: SITES, SYSTEMS, C_PLANTS, LITERATURE, C_ORG, CELLS, COUNTRY,LITERATURE
con2=dbConnect(PostgreSQL(),user = "postgres",password="wetlands",dbname = "postgres")
SITES <- as.data.frame(dbGetQuery(con2, 'SELECT * from "SITES"'))
SITES$CTRY_CODE[is.na(SITES$CTRY_CODE)=="TRUE"]<-"NULL"
SITES$MUNI[is.na(SITES$MUNI)=="TRUE"]<-"NULL"
SITES$LONG[is.na(SITES$LONG)=="TRUE"]<-0
SYSTEMS<-as.data.frame(dbGetQuery(con2, 'SELECT * from "SYSTEMS"'))
C_PLANTS<-as.data.frame(dbGetQuery(con2, 'SELECT * from "C_PLANTS"'))
C_PLANTS$PLANT_SPEC[is.na(C_PLANTS$PLANT_SPEC)=="TRUE"]<-"NULL"
LITERATURE<-as.data.frame(dbGetQuery(con2, 'SELECT * from "LITERATURE"'))
LITERATURE$TITLE[is.na(LITERATURE$TITLE)=="TRUE"]<-"NULL"
C_ORG<-as.data.frame(dbGetQuery(con2, 'SELECT * from "C_ORG"'))
CELLS<-as.data.frame(dbGetQuery(con2, 'SELECT * from "CELLS"'))
COUNTRY<-as.data.frame(dbGetQuery(con2, 'SELECT * from "COUNTRY"'))
LITERATURE_comparison<-data.frame(LIT_ID=LITERATURE$LIT_ID,TITLE=LITERATURE$TITLE)
C_PLANTS_comparison<-data.frame(PLANT_ID=C_PLANTS$PLANT_ID,PLANT_SPEC=C_PLANTS$PLANT_SPEC)
SITES_comparison<-data.frame(SITE_ID=SITES$SITE_ID,CTRY_CODE=SITES$CTRY_CODE,MUNI=SITES$MUNI,LONG=SITES$LONG)

##Step 3: To read the length of the tables in the PostgreSQL database
row_C_ORG<-nrow(C_ORG)
row_LITERATURE<-nrow(LITERATURE)
row_PLANT<-nrow(C_PLANTS)
row_SYSTEM<-nrow(SYSTEMS)
row_SYSTEM_seq<-row_SYSTEM+1
row_C_ORG_seq<-row_C_ORG+1
row_SITES<-nrow(SITES)
b<-row_SYSTEM+1

##Step 4: To change the NA's value by "NULL"or 0 (Depending the type of character: numeric or string)
MUNICIPALITY[is.na(MUNICIPALITY)=="TRUE"]<-"NULL"
country_code[is.na(country_code)=="TRUE"]<-"NULL"
country[is.na(country)=="TRUE"]<-"NULL"
LONGITUDE[is.na(LONGITUDE)=="TRUE"]<-0
LATITUDE[is.na(LATITUDE)=="TRUE"]<-0
CITATION[is.na(CITATION)=="TRUE"]<-"NULL"
URL[is.na(URL)=="TRUE"]<-"NULL"
JOURN_NAME[is.na(JOURN_NAME)=="TRUE"]<-"NULL"
PUBLISHER[is.na(PUBLISHER)=="TRUE"]<-"NULL"
LIT_TYPE[is.na(LIT_TYPE)=="TRUE"]<-"NULL"
YEAR<-as.numeric(YEAR)
YEAR[is.na(YEAR)=="TRUE"]<-0
TITLE[is.na(TITLE)=="TRUE"]<-"NULL"
TYPE_PLANT[TYPE_PLANT==""]<-"NULL"
TYPE_WASTEWATER[is.na(TYPE_WASTEWATER)=="TRUE"]<-"NULL"
TYPE_WETLANDS[TYPE_WETLANDS==""]<-"NULL"
AREA[AREA==""]<-"NULL"

##Step 5: To declare a data.table with column names equal to the column names of the tables in the PostgreSQL database

database <- data.table( DOCUMENT=character(),
                        CTRY_CODE=character(),
                        SYSTEM_ID=character(),
                        SITE_ID=character(),
                        LIT_ID=character(),
                        PLANT_ID=character(),
                        CELL_ID=character(),
                        CTRY_NAME=character(),
                        C_ORG_ID=character(),
                        MUNI=character(), 
                        LONG=numeric(),
                        LAT=numeric(),
                        LIT_TYPE=character(),
                        TITLE=character(),
                        YEAR=numeric(),
                        JOURN_NAME=character(),
                        PUBLISHER=character(),
                        URL=character(),
                        CELL_AREA=character(),
                        PLANT_SPEC=character(),
                        WW_TYPE=character(),
                        C_BOD_IN=numeric(),
                        C_BOD_OUT=numeric(),
                        CW_TYPE=character(),
                        stringsAsFactors=FALSE)

for (v in 1:N.docs){
  
  l1<-length(BOD_inlist[[v]])
  l2<-length(BOD_outlist[[v]])
  location<-c(l1,l2)
  max_location<-which(location == max(location), arr.ind = TRUE)
  max<-max(location)
  control<-0
  
  if(length(max_location)==2){
    
    LIT_ID<-vector(mode="character",length=max)
    lit_type<-rep(LIT_TYPE[v],max)
    url<-rep(URL[v],max)
    year<-rep(YEAR[v],max)
    journ_name<-rep(JOURN_NAME[v],max)
    publisher<-rep(PUBLISHER[v],max)
    Country<-rep(country[v],max)
    ctr_code<-rep(country_code[v],max)
    longitude<-rep(LONGITUDE[v],max)
    latitude<-rep(LATITUDE[v],max)
    typewetlands<-rep(TYPE_WETLANDS[v],max)
    typewaster<-rep(TYPE_WASTEWATER[v],max)
    typeplant<-rep(TYPE_PLANT[v],max)
    municipality<-rep(MUNICIPALITY[v],max)
    area<-rep(AREA[v],max)
    bod_outlist<-rep(BOD_outlist[[v]],max)
    bod_inlist<-rep(BOD_inlist[[v]],max)
    document<-rep(my.docs$names[v],max)
    SITE_ID<-vector(mode="character",length=max)
    PLANT_ID<-vector(mode="character",length=max)
    SYSTEM_ID<-vector(mode="character",length=max)
    C_ORG_ID<-vector(mode="character",length=max)
    CELL_ID<-vector(mode="character",length=max)
    row_SYSTEM_end<-row_SYSTEM+max
    cont<-0
    for (r in row_SYSTEM_seq:row_SYSTEM_end){
    cont<-cont+1
    SYSTEM_ID[cont]<-paste0("SYSTEM_",b)
    C_ORG_ID[cont]<-paste0("C_ORG_ID_",r)
    CELL_ID[cont]<-paste0("CELL_",r)
    }
    b<-b+1
    row_SYSTEM<-row_SYSTEM_end
    row_SYSTEM_seq<-row_SYSTEM+1
    title<-rep(TITLE[v],max)
 
    for (i in 1:max){
      
      row_SITES_comparison<-nrow(SITES_comparison)
      
      for (m in 1:row_SITES_comparison){
          
          if ((SITES_comparison$CTRY_CODE[m]==ctr_code[i])&(SITES_comparison$MUNI[m]==municipality[i])&(SITES_comparison$LONG[m]==longitude[i])){
            temporal_sites<-as.vector(SITES_comparison$SITE_ID)
            SITE_ID[i]<-temporal_sites[m]
        }
      }
        
        if (SITE_ID[i]==""){
          row_SITES_seq<-row_SITES_comparison+1
          SITE_ID[i]<-paste0("SITE_",row_SITES_seq)
          add_SITES<-data.frame(SITE_ID=SITE_ID[i],CTRY_CODE=ctr_code[i],MUNI=municipality[i],LONG=longitude[i])
          SITES_comparison<-rbind(SITES_comparison,add_SITES)
        }
      }
    
    for (i in 1:max){
      
      row_C_PLANTS_comparison<-nrow(C_PLANTS_comparison)
      
      for (m in 1:row_C_PLANTS_comparison){
        
          if (C_PLANTS_comparison$PLANT_SPEC[m]==typeplant[i]){
            temporal_plant<-as.vector(C_PLANTS_comparison$PLANT_ID)
            PLANT_ID[i]<-temporal_plant[m]
          }
        }
        
        if (PLANT_ID[i]==""){
          row_PLANT_seq<-row_C_PLANTS_comparison+1
          PLANT_ID[i]<-paste0("PLANT_",row_PLANT_seq)
          add_PLANT<-data.frame(PLANT_ID=PLANT_ID[i],PLANT_SPEC=typeplant[i])
          C_PLANTS_comparison<-rbind(C_PLANTS_comparison,add_PLANT)
        }
      }
    
    
    for (i in 1:max){
      
      row_LITERATURE_comparison<-nrow(LITERATURE_comparison)
      
      for (m in 1:row_LITERATURE_comparison){
        
        if (LITERATURE_comparison$TITLE[m]==title[i]){
          temporal_lit<-as.vector(LITERATURE_comparison$LIT_ID)
          LIT_ID[i]<-temporal_lit[m]
        }
      }
      
      if (LIT_ID[i]==""){
        row_LITERATURE_seq<-row_LITERATURE_comparison+1
        LIT_ID[i]<-paste0("LIT_",row_LITERATURE_seq)
        add_LITERATURE<-data.frame(LIT_ID=LIT_ID[i],TITLE=title[i])
        LITERATURE_comparison<-rbind(LITERATURE_comparison,add_LITERATURE)
      }
    }
    
    database_add<-data.table(DOCUMENT=document,
                             CTRY_NAME=Country,
                             MUNI=municipality,
                             LONG=longitude,
                             LAT=latitude,
                             CELL_AREA=area,
                             PLANT_SPEC=typeplant,
                             WW_TYPE=typewaster,
                             CTRY_CODE=ctr_code,
                             SYSTEM_ID=SYSTEM_ID,
                             SITE_ID=SITE_ID,
                             LIT_ID=LIT_ID,
                             PLANT_ID=PLANT_ID,
                             CELL_ID=CELL_ID,
                             LIT_TYPE=lit_type,
                             TITLE=title,
                             YEAR=year,
                             JOURN_NAME=journ_name,
                             PUBLISHER=publisher,
                             URL=url,
                             CW_TYPE=typewetlands,
                             C_ORG_ID=C_ORG_ID,
                             C_BOD_IN=BOD_inlist[[v]],
                             C_BOD_OUT=BOD_outlist[[v]])
    
    database<-rbind(database,database_add)
    control<-1
  }
  
  if ((max_location==1)&(control!=1)){
  year<-rep(YEAR[v],max)
  lit_type<-rep(LIT_TYPE[v],max)
  url<-rep(URL[v],max)
  journ_name<-rep(JOURN_NAME[v],max)
  publisher<-rep(PUBLISHER[v],max)
  Country<-rep(country[v],max)
  ctr_code<-rep(country_code[v],max)
  title<-rep(TITLE[v],max)
  longitude<-rep(LONGITUDE[v],max)
  latitude<-rep(LATITUDE[v],max)
  typewetlands<-rep(TYPE_WETLANDS[v],max)
  typewaster<-rep(TYPE_WASTEWATER[v],max)
  typeplant<-rep(TYPE_PLANT[v],max)
  municipality<-rep(MUNICIPALITY[v],max)
  area<-rep(AREA[v],max)
  document<-rep(my.docs$names[v],max)
  SITE_ID<-vector(mode="character",length=max)
  PLANT_ID<-vector(mode="character",length=max)
  LIT_ID<-vector(mode="character",length=max)
  bod_outlist<-rep(BOD_outlist[[v]],length.out=max)
  SYSTEM_ID<-vector(mode="character",length=max)
  C_ORG_ID<-vector(mode="character",length=max)
  CELL_ID<-vector(mode="character",length=max)
  row_SYSTEM_end<-row_SYSTEM+max
  cont<-0
  
  for (r in row_SYSTEM_seq:row_SYSTEM_end){
    cont<-cont+1
    SYSTEM_ID[cont]<-paste0("SYSTEM_",b)
    C_ORG_ID[cont]<-paste0("C_ORG_ID_",r)
    CELL_ID[cont]<-paste0("CELL_",r)
  }
  b<-b+1
  row_SYSTEM<-row_SYSTEM_end
  row_SYSTEM_seq<-row_SYSTEM+1

  
  for (i in 1:max){
    
    row_SITES_comparison<-nrow(SITES_comparison)
    
    for (m in 1:row_SITES_comparison){
      
      if ((SITES_comparison$CTRY_CODE[m]==ctr_code[i])&(SITES_comparison$MUNI[m]==municipality[i])&(SITES_comparison$LONG[m]==longitude[i])){
        temporal_sites<-as.vector(SITES_comparison$SITE_ID)
        SITE_ID[i]<-temporal_sites[m]
      }
    }
    
    if (SITE_ID[i]==""){
      row_SITES_seq<-row_SITES_comparison+1
      SITE_ID[i]<-paste0("SITE_",row_SITES_seq)
      add_SITES<-data.frame(SITE_ID=SITE_ID[i],CTRY_CODE=ctr_code[i],MUNI=municipality[i],LONG=longitude[i])
      SITES_comparison<-rbind(SITES_comparison,add_SITES)
    }
  }
  
  
  for (i in 1:max){
    
    row_C_PLANTS_comparison<-nrow(C_PLANTS_comparison)
    
    for (m in 1:row_C_PLANTS_comparison){
      
      if (C_PLANTS_comparison$PLANT_SPEC[m]==typeplant[i]){
        temporal_plant<-as.vector(C_PLANTS_comparison$PLANT_ID)
        PLANT_ID[i]<-temporal_plant[m]
      }
    }
    
    if (PLANT_ID[i]==""){
      row_PLANT_seq<-row_C_PLANTS_comparison+1
      PLANT_ID[i]<-paste0("PLANT_",row_PLANT_seq)
      add_PLANT<-data.frame(PLANT_ID=PLANT_ID[i],PLANT_SPEC=typeplant[i])
      C_PLANTS_comparison<-rbind(C_PLANTS_comparison,add_PLANT)
    }
  }
  
  
  for (i in 1:max){
    
    row_LITERATURE_comparison<-nrow(LITERATURE_comparison)
    
    for (m in 1:row_LITERATURE_comparison){
      
      if (LITERATURE_comparison$TITLE[m]==title[i]){
        temporal_lit<-as.vector(LITERATURE_comparison$LIT_ID)
        LIT_ID[i]<-temporal_lit[m]
      }
    }
    
    if (LIT_ID[i]==""){
      row_LITERATURE_seq<-row_LITERATURE_comparison+1
      LIT_ID[i]<-paste0("LIT_",row_LITERATURE_seq)
      add_LITERATURE<-data.frame(LIT_ID=LIT_ID[i],TITLE=title[i])
      LITERATURE_comparison<-rbind(LITERATURE_comparison,add_LITERATURE)
    }
  }
  
  database_add<-data.table(DOCUMENT=document,
                           CTRY_NAME=Country,
                           MUNI=municipality,
                           LONG=longitude,
                           LAT=latitude,
                           CELL_AREA=area,
                           PLANT_SPEC=typeplant,
                           WW_TYPE=typewaster,
                           CTRY_CODE=ctr_code,
                           SYSTEM_ID=SYSTEM_ID,
                           SITE_ID=SITE_ID,
                           LIT_ID=LIT_ID,
                           PLANT_ID=PLANT_ID,
                           CELL_ID=CELL_ID,
                           LIT_TYPE=lit_type,
                           TITLE=title,
                           YEAR=year,
                           JOURN_NAME=journ_name,
                           PUBLISHER=publisher,
                           URL=url,
                           CW_TYPE=typewetlands,
                           C_ORG_ID=C_ORG_ID,
                           C_BOD_IN=BOD_inlist[[v]],
                           C_BOD_OUT=bod_outlist)
  
  database<-rbind(database,database_add)
  }
  
  if ((max_location==2)&(control!=1)){
    year<-rep(YEAR[v],max)
    lit_type<-rep(LIT_TYPE[v],max)
    url<-rep(URL[v],max)
    journ_name<-rep(JOURN_NAME[v],max)
    publisher<-rep(PUBLISHER[v],max)
    Country<-rep(country[v],max)
    title<-rep(TITLE[v],max)
    ctr_code<-rep(country_code[v],max)
    longitude<-rep(LONGITUDE[v],max)
    latitude<-rep(LATITUDE[v],max)
    typewetlands<-rep(TYPE_WETLANDS[v],max)
    typewaster<-rep(TYPE_WASTEWATER[v],max)
    typeplant<-rep(TYPE_PLANT[v],max)
    municipality<-rep(MUNICIPALITY[v],max)
    area<-rep(AREA[v],max)
    document<-rep(my.docs$names[v],max)
    SITE_ID<-vector(mode="character",length=max)
    PLANT_ID<-vector(mode="character",length=max)
    LIT_ID<-vector(mode="character",length=max)
    bod_inlist<-rep(BOD_inlist[[v]],length.out=max)
    SYSTEM_ID<-vector(mode="character",length=max)
    C_ORG_ID<-vector(mode="character",length=max)
    CELL_ID<-vector(mode="character",length=max)
    row_SYSTEM_end<-row_SYSTEM+max
    cont<-0
    for (r in row_SYSTEM_seq:row_SYSTEM_end){
      cont<-cont+1
      SYSTEM_ID[cont]<-paste0("SYSTEM_",b)
      C_ORG_ID[cont]<-paste0("C_ORG_ID_",r)
      CELL_ID[cont]<-paste0("CELL_",r)
    }
    b+1
    row_SYSTEM<-row_SYSTEM_end
    row_SYSTEM_seq<-row_SYSTEM+1
    
    for (i in 1:max){
      
      row_SITES_comparison<-nrow(SITES_comparison)
      
      for (m in 1:row_SITES_comparison){
        
        if ((SITES_comparison$CTRY_CODE[m]==ctr_code[i])&(SITES_comparison$MUNI[m]==municipality[i])&(SITES_comparison$LONG[m]==longitude[i])){
          temporal_sites<-as.vector(SITES_comparison$SITE_ID)
          SITE_ID[i]<-temporal_sites[m]
        }
      }
      
      if (SITE_ID[i]==""){
        row_SITES_seq<-row_SITES_comparison+1
        SITE_ID[i]<-paste0("SITE_",row_SITES_seq)
        add_SITES<-data.frame(SITE_ID=SITE_ID[i],CTRY_CODE=ctr_code[i],MUNI=municipality[i],LONG=longitude[i])
        SITES_comparison<-rbind(SITES_comparison,add_SITES)
      }
    }
    
    
    for (i in 1:max){
      
      row_C_PLANTS_comparison<-nrow(C_PLANTS_comparison)
      
      for (m in 1:row_C_PLANTS_comparison){
        
        if (C_PLANTS_comparison$PLANT_SPEC[m]==typeplant[i]){
          temporal_plant<-as.vector(C_PLANTS_comparison$PLANT_ID)
          PLANT_ID[i]<-temporal_plant[m]
        }
      }
      
      if (PLANT_ID[i]==""){
        row_PLANT_seq<-row_C_PLANTS_comparison+1
        PLANT_ID[i]<-paste0("PLANT_",row_PLANT_seq)
        add_PLANT<-data.frame(PLANT_ID=PLANT_ID[i],PLANT_SPEC=typeplant[i])
        C_PLANTS_comparison<-rbind(C_PLANTS_comparison,add_PLANT)
      }
    }
    
    
    for (i in 1:max){
      
      row_LITERATURE_comparison<-nrow(LITERATURE_comparison)
      
      for (m in 1:row_LITERATURE_comparison){
        
        if (LITERATURE_comparison$TITLE[m]==title[i]){
          temporal_lit<-as.vector(LITERATURE_comparison$LIT_ID)
          LIT_ID[i]<-temporal_lit[m]
        }
      }
      
      if (LIT_ID[i]==""){
        row_LITERATURE_seq<-row_LITERATURE_comparison+1
        LIT_ID[i]<-paste0("LIT_",row_LITERATURE_seq)
        add_LITERATURE<-data.frame(LIT_ID=LIT_ID[i],TITLE=title[i])
        LITERATURE_comparison<-rbind(LITERATURE_comparison,add_LITERATURE)
      }
    }
    database_add<-data.table(DOCUMENT=document,
                             CTRY_NAME=Country,
                             MUNI=municipality,
                             LONG=longitude,
                             LAT=latitude,
                             CELL_AREA=area,
                             PLANT_SPEC=typeplant,
                             WW_TYPE=typewaster,
                             CTRY_CODE=ctr_code,
                             SYSTEM_ID=SYSTEM_ID,
                             SITE_ID=SITE_ID,
                             LIT_ID=LIT_ID,
                             PLANT_ID=PLANT_ID,
                             CELL_ID=CELL_ID,
                             LIT_TYPE=lit_type,
                             TITLE=title,
                             YEAR=year,
                             JOURN_NAME=journ_name,
                             PUBLISHER=publisher,
                             URL=url,
                             CW_TYPE=typewetlands,
                             C_ORG_ID=C_ORG_ID,
                             C_BOD_IN=bod_inlist,
                             C_BOD_OUT=BOD_outlist[[v]])
    
    database<-rbind(database,database_add)
  }  
}

database[database=="NULL"]<-NA
database[database==0]<-NA
```

```{r}
head(database)
```



## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
