part1<-sub("(.*?)[,]","",publication_year[1])
part2<-sub("[,](.*)","",part1)
YEAR[m]<-str_sub(part2, start= -4)
}
YEAR
library(RJSONIO)
library(RCurl)
install.packages("RJSONIO")
library(RJSONIO)
library(RCurl)
getGeoData <- function(location){
location <- gsub(' ','+',location)
geo_data <- getURL(paste("https://maps.googleapis.com/maps/api/geocode/json?address=",location, sep=""))
raw_data_2 <- fromJSON(geo_data)
return(raw_data_2)
}
getGeoData("San Francisco")
f<-getGeoData("San Francisco")
f
coordinates<-geocode"San Francisco")
coordinates<-geocode("San Francisco")
coordinates
str(f)
g<-f
g<-unlist(g)
g
g[1]
str(g)
g[14]
g[22]
g[23]
r<-sub("results.geometry.location.lng","",g[23])
r
g[23]
str(g[23])
f<-g[23]
f
f<-unname(f)
f
f2<-unname(g[22])
f2
coordinates<-geocode("Barranquilla")
coordinates
f<-getGeoData("Barranquilla")
f
f<-unlist(f)
f[22]
f[23]
coordinates<-geocode("Cartagena")
coordinates
coordinates<-geocode("Barranquilla")
f<-getGeoData("Barranquilla")
f<-unlist(f)
f[22]
LONGITUDE<-c(rep(NA,N.docs))
LATITUDE<-c(rep(NA,N.docs))
m<-1
coordinates<-unlist(getGeoData("Barranquilla"))
LONGITUDE[m]<-unname(coordinates[23])
LATITUDE[m]<-unname(coordinates[22])
LONGITUDE
coordinates
coordinates<-unlist(getGeoData("Rome"))
LONGITUDE[m]<-unname(coordinates[23])
LATITUDE[m]<-unname(coordinates[22])
coordinates
LONGITUDE<-c(rep(NA,N.docs))
LATITUDE<-c(rep(NA,N.docs))
getGeoData <- function(location){
location <- gsub(' ','+',location)
geo_data <- getURL(paste("https://maps.googleapis.com/maps/api/geocode/json?key=AbCdEfGhIjKlMnOpQrStUvWxYz&address=Dallas&sensor=true",location, sep=""))
raw_data_2 <- fromJSON(geo_data)
return(raw_data_2)
}
coordinates<-unlist(getGeoData("Rome"))
LONGITUDE[m]<-unname(coordinates[23])
LATITUDE[m]<-unname(coordinates[22])
coordinates
LONGITUDE<-c(rep(NA,N.docs))
LATITUDE<-c(rep(NA,N.docs))
getGeoData <- function(location){
location <- gsub(' ','+',location)
geo_data <- getURL(paste("https://maps.googleapis.com/maps/api/geocode/json?key=AIzaSyCs_UUrHnmz-PW5dfe020gXtROXHpEWrq8&address=Dallas&sensor=true",location, sep=""))
raw_data_2 <- fromJSON(geo_data)
return(raw_data_2)
}
coordinates<-unlist(getGeoData("Rome"))
LONGITUDE[m]<-unname(coordinates[23])
LATITUDE[m]<-unname(coordinates[22])
coordinates
LONGITUDE
LATITUDE
library(gsubf)
library(proto)
library(RSQLite)
library(readr)
library(tm)
library(SnowballC)
library(RWeka)
library(sqldf)
library(dplyr)
library(data.table)
library(ggmap)
library(RODBC)
library(XML)
library(rvest)
library(xml2)
library(openxlsx)
library(RPostgreSQL)
library(stringr)
library(data.table)
library(stringr)
rm(list=ls())
##PRE - PROCESSING OF DOCUMENTS
## Step 1: To convert the document from PDF to .txt by opening the PDF file in Microsoft WORD and then save it as .txt
## Step 2: To eliminate whitespaces and tap spaces by using the program Note++: (1) To enter tool "Search" (2) To enter tool "Replace" (3) To write the regular expression [\t\r\n\s]+ in the field: "Find what"
## Step 3: To name the documents in the form: doc[#].txt e.g. doc1.txt
## Step 4: To copy the processed files in the folder "C:/Users/xxx/Desktop/WETLANDS_EXTRATION/Documents"
## Step 5: To set the working directory with the location of the files - Folder: "C:/xxx/xxx/xxx/WETLANDS_EXTRATION/Documents"
setwd("C:/Users/Mauricio/Desktop/WETLANDS_EXTRATION/Documents")
##Extracted parameters:
##1- Parameter_1: Location - Country
##2- Parameter_2: Location - Country
##3- Parameter_3: Location - Country
##4- Parameter_3: Location - Country
##5- Parameter_3: Location - Country
##CLEANING AND CONSTRUCTION OF TEXT MATRIX
## Step 1: To read the files - Folder: "C:/Users/xxx/Desktop/WETLANDS_EXTRATION/Documents"
files_names<-list.files(getwd())
doc.list<-lapply(files_names,read_file)
N.docs<-length(doc.list)
## Step 2: To construct Corpus of documents
my.docs<-VectorSource(c(doc.list))
my.docs$names<-sub(".txt","",files_names)
mydocs<-Corpus(my.docs)
## Step 3: To clean special characters
removeSpecialChars<-function(x) gsub("[^â€“a-zA-z0-9. ]","",x)
mydocs<-tm_map(mydocs,removeSpecialChars)
## Step 4: To clean stopwords()
mydocs<-tm_map(mydocs,removeWords,stopwords('en'))
## Step 5: To clean doble whitespaces between words
mydocs<-tm_map(mydocs,stripWhitespace)
## Step 6: To clean information before the word "Abstract"
removeAbstract<-function(x) gsub("(.*?)A(?i)bstract(?-i)","",x)
mydocs<-tm_map(mydocs,removeAbstract)
## Step 7: To clean information after the word "Conclusions"
removeConclusions<-function(x) sub("C(?i)onclusion(?-i)(.*)","",x)
mydocs<-tm_map(mydocs,removeConclusions)
## Step 8: To divide the text in four main parts: Abstract, Introduction, Materials and Methods and Results
## Step 8.1: Abstract
removeIntroduction_down<-function(x) sub("I(?i)ntroduction(?-i)(.*)","",x)
str1<-tm_map(mydocs,removeIntroduction_down)
## Step 8.2: Introduction
removeIntroduction_up<-function(x) sub("(.*?)I(?i)ntroduction(?-i)","",x)
str2_fs1.1<-tm_map(mydocs,removeIntroduction_up)
removeMethods_down<-function(x) sub("M(?i)ethods(?-i)(.*)","",x)
str2_fs1.2<-tm_map(str2_fs1.1,removeMethods_down)
removeMethods_down<-function(x) sub("M(?i)aterial(?-i)(.*)","",x)
str2<-tm_map(str2_fs1.2,removeMethods_down)
## Step 8.3: Materials and Methods
removeMethods_up<-function(x) sub("(.*?)M(?i)aterial(?-i)","",x)
str3_fs1.1<-tm_map(mydocs,removeMethods_up)
removeMethods_up<-function(x) sub("(.*?)M(?i)ethods(?-i)","",x)
str3_fs1.2<-tm_map(str3_fs1.1,removeMethods_up)
removeResults_down<-function(x) sub("R(?i)esults(?-i)(.*)","",x)
str3<-tm_map(str3_fs1.2,removeResults_down)
## Step 8.4: Results
removeResults_up<-function(x) sub("(.*?)R(?i)esults(?-i)","",x)
str4<-tm_map(mydocs,removeResults_up)
## Step 9: To save information in data.frames for posterior construction of VCorpus
dataframe_docs<-data.frame(text=sapply(mydocs, as.character),stringsAsFactors = FALSE)
extract_str1<-data.frame(text=sapply(str1, as.character),stringsAsFactors = FALSE)
extract_str2<-data.frame(text=sapply(str2, as.character),stringsAsFactors = FALSE)
extract_str3<-data.frame(text=sapply(str3, as.character),stringsAsFactors = FALSE)
extract_str4<-data.frame(text=sapply(str4, as.character),stringsAsFactors = FALSE)
## Step 10: To create VCorpus for N-gram analysis
df<-data.frame(doc_id=seq(1:N.docs),text=dataframe_docs$text,stringsAsFactors = F)
VCorpus_docs<-VCorpus(DataframeSource(df))
df_str1<-data.frame(doc_id=seq(1:N.docs),text=extract_str1$text,stringsAsFactors = F)##Abstract
VCorpus_docs1<-VCorpus(DataframeSource(df_str1))
df_str2<-data.frame(doc_id=seq(1:N.docs),text=extract_str2$text,stringsAsFactors = F)##Introduction
VCorpus_docs2<-VCorpus(DataframeSource(df_str2))
df_str3<-data.frame(doc_id=seq(1:N.docs),text=extract_str3$text,stringsAsFactors = F)##Materials and methods
VCorpus_docs3<-VCorpus(DataframeSource(df_str3))
df_str4<-data.frame(doc_id=seq(1:N.docs),text=extract_str4$text,stringsAsFactors = F)##Results
VCorpus_docs4<-VCorpus(DataframeSource(df_str4))
##PARAMETER_1 AND PARAMETER_2: COUNTRY AND COUNTRY_CODE
## Step 1: To read the dataset of countries in the world - Folder: "C:/xxx/xxx/xxx/WETLANDS_EXTRATION/Documents_backup"
setwd("C:/Users/Mauricio/Desktop/WETLANDS_EXTRATION/Documents_backup")
countries<-read.delim("GEODATASOURCE-COUNTRY.txt", header = TRUE, sep = "\t")
countries_names<-as.vector(countries[,4])
countries_codes<-as.vector(countries[,1])
## Step 2: To extract the number of countries in the dataset
l_countries<-length(countries_names)
country_code<- c(rep(NA,N.docs))
country<-c(rep(NA,N.docs))
## Step 3: To extract the country name
## Rules
## 1- SP's: Abstract, Materials and Methods
## 2- Tokanizer: min=1, max=2
for (m in 1:N.docs){
Tokenizer<-function(x) NGramTokenizer(x,Weka_control(min=1,max=2))
head(NGramTokenizer(VCorpus_docs3[m],Weka_control(min=1,max=2)))
dtm3<-TermDocumentMatrix(VCorpus_docs3[m],control=list(tokenize=Tokenizer,tolower=FALSE))
bio3<-as.data.frame(t(as.matrix(dtm3)))
keywords3<-colnames(bio3)
kw_country3<-as.data.frame(keywords3)
colnames(kw_country3)[1]<-"keywords_country"
Tokenizer<-function(x) NGramTokenizer(x,Weka_control(min=1,max=2))
head(NGramTokenizer(VCorpus_docs1[m],Weka_control(min=1,max=2)))
dtm1<-TermDocumentMatrix(VCorpus_docs1[m],control=list(tokenize=Tokenizer,tolower=FALSE))
bio1<-as.data.frame(t(as.matrix(dtm1)))
keywords1<-colnames(bio1)
kw_country1<-as.data.frame(keywords1)
colnames(kw_country1)[1]<-"keywords_country"
kw_country<-rbind(kw_country3,kw_country1)
l_keywords<-length(keywords1)+length(keywords3)
extract_country<-as.data.frame(lapply(kw_country, function(x) lapply(countries_names,function(y) grepl(y,x))))
for (i in 1:l_keywords){
for (j in 1:l_countries){
if (extract_country[i,j]=="TRUE"){
country[m]<-countries_names[j]
country_code[m]<-countries_codes[j]
}
}
}
}
##PARAMETER_3: MUNICIPALITY
## Step 1: To read the dataset of cities in the world - Folder: "C:/xxx/xxx/xxx/WETLANDS_EXTRATION/Documents_backup"
cities<-fread("GEODATASOURCE-CITIES-FREE.txt", header = TRUE, sep = "\t")
cities_data<-as.data.frame(cities)
## Step 2: To extract the parameter City_name
## Rules
## 1- SP's: Materials and Methods
## 2- Tokanizer: min=1, max=4
MUNICIPALITY<-c(rep(NA,N.docs))
for (m in 1:N.docs){
if (is.na(country_code[m])==FALSE){
Tokenizer_city<-function(x) NGramTokenizer(x,Weka_control(min=1,max=4))
head(NGramTokenizer(VCorpus_docs3[m],Weka_control(min=1,max=4)))
dtm_city<-TermDocumentMatrix(VCorpus_docs3[m],control=list(tokenize=Tokenizer_city,tolower=FALSE))
bio_city<-as.data.frame(t(as.matrix(dtm_city)))
keywords_city<-colnames(bio_city)
kw_city<-as.vector(keywords_city)
l_keywords_city<-length(keywords_city)
filtration_cities<-subset(cities_data,CC_FIPS==country_code[m])
filtration_cities<-as.vector(filtration_cities[,2])
l_cities<-length(filtration_cities)
filtration_cities<-paste("^",filtration_cities,"$",sep="")
extract_cities<-lapply(filtration_cities,function(x)grep(x,kw_city,value=TRUE))
extract_cities<-extract_cities[lapply(extract_cities,length)>0]
if ((NROW(extract_cities)==1)&(length(extract_cities[[1]])==1)){
MUNICIPALITY[m]<-extract_cities[[1]][1]
}
}
rm(dtm_city,bio_city,keywords_city,kw_city,l_keywords_city,extract_cities,filtration_cities)
}
##PARAMETER_4: COORDINATES (LATITUDE,LONGITUDE)
##Step 1.1: To extract coordinates using API from Gogle Maps based on the name of the city (MUNICIPALITY)
LONGITUDE<-c(rep(NA,N.docs))
LATITUDE<-c(rep(NA,N.docs))
getGeoData <- function(location){
location <- gsub(' ','+',location)
geo_data <- getURL(paste("https://maps.googleapis.com/maps/api/geocode/json?key=AIzaSyCs_UUrHnmz-PW5dfe020gXtROXHpEWrq8&address=Dallas&sensor=true",location, sep=""))
raw_data_2 <- fromJSON(geo_data)
return(raw_data_2)
}
for (m in 1:N.docs){
if (is.na(MUNICIPALITY[m])=="FALSE"){
coordinates<-unlist(getGeoData(MUNICIPALITY[m]))
LONGITUDE[m]<-unname(coordinates[23])
LATITUDE[m]<-unname(coordinates[22])
}
##Step 1.2: To extract coordinates directly from the text
## Rules
## 1- SP's: Materials and Methods
## 2- Tokanizer: min=1, max=1
if (is.na(MUNICIPALITY[m])=="TRUE"){
Tokenizer<-function(x) NGramTokenizer(x,Weka_control(min=1,max=1))
head(NGramTokenizer(VCorpus_docs3[m],Weka_control(min=1,max=1)))
dtm_coordinates<-TermDocumentMatrix(VCorpus_docs3[m],control=list(tokenize=Tokenizer,tolower=FALSE))
bio_coordinates<-as.data.frame(t(as.matrix(dtm_coordinates)))
keywords_coordinates<-colnames(bio_coordinates)
kw_coordinates<-as.data.frame(keywords_coordinates)
longitud_filter_E<-as.vector(filter(kw_coordinates, grepl(pattern="^[0-9]{5}E",kw_coordinates$keywords_coordinates))[1,])
longitud_filter_W<-as.vector(filter(kw_coordinates, grepl(pattern="^[0-9]{5}W",kw_coordinates$keywords_coordinates))[1,])
latitud_filter_N<-as.vector(filter(kw_coordinates, grepl(pattern="^[0-9]{5}N",kw_coordinates$keywords_coordinates))[1,])
latitud_filter_S<-as.vector(filter(kw_coordinates, grepl(pattern="^[0-9]{5}S",kw_coordinates$keywords_coordinates))[1,])
if (is.na(longitud_filter_E[1])=="FALSE"){
lon_number<-sub("(^[^0-9]*)(\\d+)([^0-9].*)", "\\2", longitud_filter_E[1])
grade_longitud<-as.numeric(sub('([0-9]{2}).*', '\\1', lon_number))
minutes_longitud<-as.numeric(sub(".*(\\d+{2}).*$", "\\1", lon_number))
LONGITUDE[m]<-grade_longitud+minutes_longitud/60
}
if (is.na(latitud_filter_N[1])=="FALSE"){
lat_number<-sub("(^[^0-9]*)(\\d+)([^0-9].*)", "\\2", latitud_filter_N[1])
grade_latitud<-as.numeric(sub('([0-9]{2}).*', '\\1', lat_number))
minutes_latitud<-as.numeric(sub(".*(\\d+{2}).*$", "\\1", lat_number))
LATITUDE[m]<-grade_latitud+minutes_latitud/60
}
if (is.na(longitud_filter_W[1])=="FALSE"){
lon_number<-sub("(^[^0-9]*)(\\d+)([^0-9].*)", "\\2", longitud_filter_W[1])
grade_longitud<-as.numeric(sub('([0-9]{2}).*', '\\1', lon_number))
minutes_longitud<-as.numeric(sub(".*(\\d+{2}).*$", "\\1", lon_number))
LONGITUDE[m]<-(grade_longitud+minutes_longitud/60)*(-1)
}
if (is.na(latitud_filter_S[1])=="FALSE"){
lat_number<-sub("(^[^0-9]*)(\\d+)([^0-9].*)", "\\2", latitud_filter_S[1])
grade_latitud<-as.numeric(sub('([0-9]{2}).*', '\\1', lat_number))
minutes_latitud<-as.numeric(sub(".*(\\d+{2}).*$", "\\1", lat_number))
LATITUDE[m]<-(grade_latitud+minutes_latitud/60)*-1
}
}
}
LATITUDE
LONGITUDE
MUNICIPALITY
LONGITUDE<-c(rep(NA,N.docs))
LATITUDE<-c(rep(NA,N.docs))
getGeoData <- function(location){
location <- gsub(' ','+',location)
geo_data <- getURL(paste("https://maps.googleapis.com/maps/api/geocode/json?key=AIzaSyCs_UUrHnmz-PW5dfe020gXtROXHpEWrq8&address=Dallas&sensor=true",location, sep=""))
raw_data_2 <- fromJSON(geo_data)
return(raw_data_2)
}
N.docs
for (m in 1:N.docs){
if (is.na(MUNICIPALITY[m])=="FALSE"){
coordinates<-unlist(getGeoData(MUNICIPALITY[m]))
LONGITUDE[m]<-unname(coordinates[23])
LATITUDE[m]<-unname(coordinates[22])
}
}
LONGITUDE
LATITUDE
LONGITUDE<-c(rep(NA,N.docs))
LATITUDE<-c(rep(NA,N.docs))
MUNICIPALITY
coordinates<-unlist(getGeoData("Tunis"))
LONGITUDE[m]<-unname(coordinates[23])
LATITUDE[m]<-unname(coordinates[22])
LONGITUDE<-c(rep(NA,N.docs))
LATITUDE<-c(rep(NA,N.docs))
m<-1
coordinates<-unlist(getGeoData(MUNICIPALITY[m]))
LONGITUDE[m]<-unname(coordinates[23])
LATITUDE[m]<-unname(coordinates[22])
LONGITUDE
LATITUDE
m<-2
coordinates<-unlist(getGeoData("Marmara"))
LONGITUDE[m]<-unname(coordinates[23])
LATITUDE[m]<-unname(coordinates[22])
LONGITUDE
LATITUDE
f<-geocode("Marmara")
f
coordinates<-unlist(getGeoData("Marmara"))
coordinates
LONGITUDE<-c(rep(NA,N.docs))
LATITUDE<-c(rep(NA,N.docs))
getGeoData <- function(location){
location <- gsub(' ','+',location)
geo_data <- getURL(paste("https://maps.googleapis.com/maps/api/geocode/json?key=AIzaSyCs_UUrHnmz-PW5dfe020gXtROXHpEWrq8",location, sep=""))
raw_data_2 <- fromJSON(geo_data)
return(raw_data_2)
}
coordinates<-unlist(getGeoData("Marmara"))
LONGITUDE[1]<-unname(coordinates[23])
LATITUDE[1]<-unname(coordinates[22])
LATITUDE
coordinates
LONGITUDE<-c(rep(NA,N.docs))
LATITUDE<-c(rep(NA,N.docs))
getGeoData <- function(location){
location <- gsub(' ','+',location)
geo_data <- getURL(paste("https://maps.googleapis.com/maps/api/geocode/json?address=",location,"&key=AIzaSyCs_UUrHnmz-PW5dfe020gXtROXHpEWrq8", sep=""))
raw_data_2 <- fromJSON(geo_data)
return(raw_data_2)
}
coordinates<-unlist(getGeoData("Marmara"))
LONGITUDE[1]<-unname(coordinates[23])
LATITUDE[1]<-unname(coordinates[22])
LATITUDE
coordinates
unname(coordinates[23])
str(coordinates)
name(coordinates[1])
names(coordinates)
names(coordinates)[1]
LONGITUDE<-c(rep(NA,N.docs))
LATITUDE<-c(rep(NA,N.docs))
getGeoData <- function(location){
location <- gsub(' ','+',location)
geo_data <- getURL(paste("https://maps.googleapis.com/maps/api/geocode/json?address=",location,"&key=AIzaSyCs_UUrHnmz-PW5dfe020gXtROXHpEWrq8", sep=""))
raw_data_2 <- fromJSON(geo_data)
return(raw_data_2)
}
m<-1
coordinates<-unlist(getGeoData("Marmara")
for (a in 1:length(coordinates)){
if (names(coordinates)[a]=="results.geometry.location.lat"){
LATITUDE[m]<-unname(coordinates[a])
}
if (names(coordinates)[a]=="results.geometry.location.lng"){
LONGITUDE[m]<-unname(coordinates[a])
}
}
coordinates<-unlist(getGeoData("Marmara")
for (a in 1:length(coordinates)){
if (names(coordinates)[a]=="results.geometry.location.lat"){
LATITUDE[m]<-unname(coordinates[a])
}
if (names(coordinates)[a]=="results.geometry.location.lng"){
LONGITUDE[m]<-unname(coordinates[a])
}
}
length(coordinates)
coordinates<-unlist(getGeoData("Marmara"))
for (a in 1:length(coordinates)){
if (names(coordinates)[a]=="results.geometry.location.lat"){
LATITUDE[m]<-unname(coordinates[a])
}
if (names(coordinates)[a]=="results.geometry.location.lng"){
LONGITUDE[m]<-unname(coordinates[a])
}
}
LONGITUDE
LATITUDE
LONGITUDE<-c(rep(NA,N.docs))
LATITUDE<-c(rep(NA,N.docs))
getGeoData <- function(location){
location <- gsub(' ','+',location)
geo_data <- getURL(paste("https://maps.googleapis.com/maps/api/geocode/json?address=",location,"&key=AIzaSyCs_UUrHnmz-PW5dfe020gXtROXHpEWrq8", sep=""))
raw_data_2 <- fromJSON(geo_data)
return(raw_data_2)
}
for (m in 1:N.docs){
if (is.na(MUNICIPALITY[m])=="FALSE"){
coordinates<-unlist(getGeoData(MUNICIPALITY[m]))
for (a in 1:length(coordinates)){
if (names(coordinates)[a]=="results.geometry.location.lat"){
LATITUDE[m]<-unname(coordinates[a])
}
if (names(coordinates)[a]=="results.geometry.location.lng"){
LONGITUDE[m]<-unname(coordinates[a])
}
}
}
##Step 1.2: To extract coordinates directly from the text
## Rules
## 1- SP's: Materials and Methods
## 2- Tokanizer: min=1, max=1
if (is.na(MUNICIPALITY[m])=="TRUE"){
Tokenizer<-function(x) NGramTokenizer(x,Weka_control(min=1,max=1))
head(NGramTokenizer(VCorpus_docs3[m],Weka_control(min=1,max=1)))
dtm_coordinates<-TermDocumentMatrix(VCorpus_docs3[m],control=list(tokenize=Tokenizer,tolower=FALSE))
bio_coordinates<-as.data.frame(t(as.matrix(dtm_coordinates)))
keywords_coordinates<-colnames(bio_coordinates)
kw_coordinates<-as.data.frame(keywords_coordinates)
longitud_filter_E<-as.vector(filter(kw_coordinates, grepl(pattern="^[0-9]{5}E",kw_coordinates$keywords_coordinates))[1,])
longitud_filter_W<-as.vector(filter(kw_coordinates, grepl(pattern="^[0-9]{5}W",kw_coordinates$keywords_coordinates))[1,])
latitud_filter_N<-as.vector(filter(kw_coordinates, grepl(pattern="^[0-9]{5}N",kw_coordinates$keywords_coordinates))[1,])
latitud_filter_S<-as.vector(filter(kw_coordinates, grepl(pattern="^[0-9]{5}S",kw_coordinates$keywords_coordinates))[1,])
if (is.na(longitud_filter_E[1])=="FALSE"){
lon_number<-sub("(^[^0-9]*)(\\d+)([^0-9].*)", "\\2", longitud_filter_E[1])
grade_longitud<-as.numeric(sub('([0-9]{2}).*', '\\1', lon_number))
minutes_longitud<-as.numeric(sub(".*(\\d+{2}).*$", "\\1", lon_number))
LONGITUDE[m]<-grade_longitud+minutes_longitud/60
}
if (is.na(latitud_filter_N[1])=="FALSE"){
lat_number<-sub("(^[^0-9]*)(\\d+)([^0-9].*)", "\\2", latitud_filter_N[1])
grade_latitud<-as.numeric(sub('([0-9]{2}).*', '\\1', lat_number))
minutes_latitud<-as.numeric(sub(".*(\\d+{2}).*$", "\\1", lat_number))
LATITUDE[m]<-grade_latitud+minutes_latitud/60
}
if (is.na(longitud_filter_W[1])=="FALSE"){
lon_number<-sub("(^[^0-9]*)(\\d+)([^0-9].*)", "\\2", longitud_filter_W[1])
grade_longitud<-as.numeric(sub('([0-9]{2}).*', '\\1', lon_number))
minutes_longitud<-as.numeric(sub(".*(\\d+{2}).*$", "\\1", lon_number))
LONGITUDE[m]<-(grade_longitud+minutes_longitud/60)*(-1)
}
if (is.na(latitud_filter_S[1])=="FALSE"){
lat_number<-sub("(^[^0-9]*)(\\d+)([^0-9].*)", "\\2", latitud_filter_S[1])
grade_latitud<-as.numeric(sub('([0-9]{2}).*', '\\1', lat_number))
minutes_latitud<-as.numeric(sub(".*(\\d+{2}).*$", "\\1", lat_number))
LATITUDE[m]<-(grade_latitud+minutes_latitud/60)*-1
}
}
}
LATITUDE
LONGITUDE
MUNICIPALITY
setwd("C:/Users/Mauricio/Desktop/WETLANDS_EXTRATION/Documents_backup")
data<-read.table("01_prac_WSM18_data_naumburg_saale_1995.xlsx")
data<-read.xlsx("01_prac_WSM18_data_naumburg_saale_1995.xlsx")
setwd("C:/Users/Mauricio/Desktop/WETLANDS_EXTRATION/Documents_backup/PDF")
data<-read.xlsx("01_prac_WSM18_data_naumburg_saale_1995.xlsx")
data
data_group<-grouped.data(data,breaks=c(0,25,75,350))
install.packages("grouped")
library("grouped")
data_group<-grouped.data(data,breaks=c(0,25,75,350))
library("dplyr")
library("grouped")
data_group<-grouped.data(data,breaks=c(0,25,75,350))
data<-read.xlsx("01_prac_WSM18_data_naumburg_saale_1995.xlsx")
data_group<-grouped.data(data,breaks=c(0,25,75,350))
data_group<-grouped.data(data,break=c(0,25,75,350))
str(data)
View(data)
data_group<-grouped.data(data$`Q.in.mÂ³/s`)
