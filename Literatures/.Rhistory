}
}
}
}
}
if (BOD_IN[1]==0){
for (j in 1:col){
regex_in<-grepl("BOD",colnames(table[[b]])[j],ignore.case=FALSE)&grepl("(?i)in(?-i)",table[[b]][1,j],ignore.case=FALSE)
if (regex_in=="TRUE"){
for (i in 2:row){
sec_in<-sec_in+1
BOD_IN[sec_in]<-table[[b]][i,j]
}
}
}
}
for (i in 1:row) {
regex_in<-grepl("BOD",table[[b]][i,1],ignore.case=FALSE)
if ((regex_in=="TRUE")&(is.numeric(table_numeric[i,2])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
for (u in 2:col){
sec_in<-sec_in+1
BOD_IN[sec_in]<-table[[b]][i,u]
}
}
}
BOD_IN<-BOD_IN[BOD_IN!=0]
BOD_inlist[[m]]<-BOD_IN
}
if ((con3>=1)&(con>=1)){
for (i in 1:2){
for (j in 1:col){
regex_out<-grepl("BOD",table[[b]][i,j],ignore.case=FALSE)
k<-i+1
if(k<=row){
if ((regex_out=="TRUE")&(is.numeric(table_numeric[k,j])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
p<-i+1
for (u in p:row){
sec_out<-sec_out+1
BOD_OUT[sec_out]<-table[[b]][u,j]
}
}
}
}
}
if (BOD_OUT[1]==0){
for (j in 1:col){
regex_out<-grepl("BOD",colnames(table[[b]])[j],ignore.case=FALSE)&grepl("(?i)out(?-i)",table[[b]][1,j],ignore.case=FALSE)
if (regex_out=="TRUE"){
for (i in 2:row){
sec_out<-sec_out+1
BOD_OUT[sec_out]<-table[[b]][i,j]
}
}
}
}
for (i in 1:row) {
regex_out<-grepl("BOD",table[[b]][i,1],ignore.case=FALSE)
if ((regex_out=="TRUE")&(is.numeric(table_numeric[i,2])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
for (u in 2:col){
sec_out<-sec_out+1
BOD_OUT[sec_out]<-table[[b]][i,u]
}
}
}
BOD_OUT<-BOD_OUT[BOD_OUT!=0]
BOD_outlist[[m]]<-BOD_OUT
}
if ((con4>=1)&(con>=1)){
sec<-0
for (i in 1:2){
for (j in 1:col){
regex_removal<-grepl("BOD",table[[b]][i,j],ignore.case=FALSE)
k<-i+1
if(k<=row){
if ((regex_removal=="TRUE")&(is.numeric(table_numeric[k,j])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
p<-i+1
for (u in p:row){
sec_removal<-sec_removal+1
BOD_REMOVAL[sec_removal]<-table[[b]][u,j]
}
}
}
}
}
for (i in 1:row) {
regex_removal<-grepl("BOD",table[[b]][i,1],ignore.case=FALSE)
if ((regex_removal=="TRUE")&(is.numeric(table_numeric[i,2])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
for (u in 2:col){
sec_removal<-sec_removal+1
BOD_REMOVAL[sec_removal]<-table[[b]][i,u]
}
}
}
BOD_REMOVAL<-BOD_REMOVAL[BOD_REMOVAL!=0]
BOD_removallist[[m]]<-BOD_REMOVAL
}
}
}
}
}
}
for (i in 1:N.docs){
if (length(BOD_inlist[[i]])==0){
BOD_inlist[[i]]<-c(NA)
}
if (length(BOD_outlist[[i]])==0){
BOD_outlist[[i]]<-c(NA)
}
if (length(BOD_removallist[[i]])==0){
BOD_removallist[[i]]<-c(NA)
}
BOD_removallist[[i]]<-as.numeric(BOD_removallist[[i]])
BOD_outlist[[i]]<-as.numeric(BOD_outlist[[i]])
BOD_inlist[[i]]<-as.numeric(BOD_inlist[[i]])
}
YEAR
d<-gsub("[0-9]","",YEAR[5])
d
d<-gsub("[^-a-zA-z]","",YEAR[5])
d
d<-gsub("[a-zA-z]","",YEAR[5])
d
d<-gsub("[a-zA-z ]","",YEAR[5])
d
##PARAMETER_9: BOD5 INFLUENT, BOD5 EFLUENT, JOURNAL NAME, PUBLISHER, YEAR OF PUBLICATION, TITLE
##The following approach works for pre-reviwed articles, which information is available in HTML version e.g. ELSEVIER
## Step 1: To read the dataset of links - Folder: "C:/xxx/xxx/xxx/WETLANDS_EXTRATION/HTML_LINKS"
setwd("C:/Users/Mauricio/Desktop/WETLANDS_EXTRATION/HTML_LINKS")
HTML_links<-read.xlsx("HTML_links.xlsx")
HTML_names<-as.vector(HTML_links[,1])
HTML_url<-as.vector(HTML_links[,2])
## Step 2: To set PHANTOM folder: "C:/xxx/xxx/xxx/WETLANDS_EXTRATION/Phantom/phantomjs-2.1.1-windows/bin"
setwd("C:/Users/Mauricio/Desktop/WETLANDS_EXTRATION/Phantom/phantomjs-2.1.1-windows/bin")
## Step 3: To extract the parameters
BOD_inlist<-vector(mode="list",length=N.docs)
BOD_outlist<-vector(mode="list",length=N.docs)
BOD_removallist<-vector(mode="list",length=N.docs)
CITATION<-c(rep(NA,N.docs))
URL<-c(rep(NA,N.docs))
JOURN_NAME<-c(rep(NA,N.docs))
PUBLISHER<-c(rep(NA,N.docs))
LIT_TYPE<-c(rep(NA,N.docs))
YEAR<-c(rep(NA,N.docs))
TITLE<-vector(mode="character", length=N.docs)
for (m in 1:N.docs){
LIT_TYPE[m]<-"PRE-REVIEW ARTICLE"
for (j in 1:nrow(HTML_links)){
if (my.docs$names[m]==HTML_names[j]){
url<-HTML_url[j]
URL[m]<-url
name<-paste0("doc",m,".js")
writeLines(sprintf("var page = require('webpage').create();
page.open('%s', function () {
console.log(page.content); //page source
phantom.exit();
});", url), con=name)
name_phantom<-paste0("phantomjs doc",m,".js")
name_html<-paste0("doc",m,".html")
write(readLines(pipe(name_phantom, "r")), name_html)
journal_elsevier<-grepl("sciencedirect",url,ignore.case=FALSE)##Journal: ELSEVIERR
journal_IWA<-grepl("iwaponline",url,ignore.case=FALSE)##Journal:IWA
journal_researchgate<-grepl("researchgate",url,ignore.case=FALSE)##Journal:Researchgate
journal_ncbi<-grepl("ncbi",url,ignore.case=FALSE)##Journal:NCBI
page_html<- read_html(name_html)
table<-page_html %>% html_nodes("table") %>% html_table(fill=TRUE)
head_lines<-page_html %>% html_nodes("span")%>%html_text()
if (journal_elsevier=="TRUE"){
title_article<-page_html %>% html_nodes("span.title-text")%>%html_text()
TITLE[m]<-title_article
surname<-page_html %>% html_nodes("span.text.surname")%>%html_text()
name<-page_html %>% html_nodes("span.text.given-name")%>%html_text()
doi<-page_html %>% html_nodes("a.doi")%>%html_text()
doi<-sub("(.*?)doi.org","doi:",doi)
journal_name<-page_html %>% html_nodes("h2#publication-title.publication-title")%>%html_text()
publication_year<-as.data.frame(page_html %>% html_nodes("div.text-xs")%>%html_text())
colnames(publication_year)[1]<-"YEAR"
publication_year<-as.vector(filter(publication_year,grepl(pattern="(?i)volume(?-i)",publication_year$YEAR))[,1])
name<-sub("Ã¤","ä",name)
name<-sub("Ãµ","õ",name)
name<-sub("Ã\u009c","Ü",name)
surname<-sub("Ã\u0096Ã¶","Öö",surname)
for (z in 1:length(surname)){
if (z==1){
authors<-paste0(surname[1]," ",name[1],",")
}
if ((z!=1)&(z!=length(surname))){
authors<-paste0(authors,name[z]," ",surname[z],",")
}
if(z==length(surname)){
authors<-paste0(authors,"and ",name[z]," ",surname[z],".")
}
}
PUBLISHER[m]<-authors
publication_year[1]<-sub("â\u0080\u0093","-",publication_year[1])
part1<-sub("(.*?)[,]","",publication_year[1])
part2<-sub("(.*?)[,] ","",part1)
part3<-sub("[,](.*)","",part2)
YEAR[m]<-str_sub(part3, start= -4)
p<-as.numeric(YEAR[m])
if ((is.na(p)=="TRUE")|(p<0)){
part1<-sub("(.*?)[,]","",publication_year[1])
part2<-sub("[,](.*)","",part1)
YEAR[m]<-str_sub(part2, start= -4)
}
JOURN_NAME[m]<-journal_name
CITATION[m]<-paste0(authors,'"',title_article,'"',".",journal_name," ",publication_year[1],".",doi)
}
if (journal_IWA=="TRUE"){
title_article<-page_html %>% html_nodes("h1#page-title.highwire-cite-title")%>%html_text()
TITLE[m]<-title_article
journal_name<-page_html %>% html_nodes("div.region-inner.region-branding-inner")%>%html_text()
JOURN_NAME[m]<-journal_name
name<-page_html %>% html_nodes("span.highwire-citation-authors")%>%html_text()
PUBLISHER[m]<-name[1]
doi_publicationyear<-page_html %>% html_nodes("div.highwire-cite-metadata")%>%html_text()
CITATION[m]<-paste0(name[1],".",'"',title_article,'"',".Water Sci Technol.",doi_publicationyear[1])
}
if (journal_ncbi=="TRUE"){
journal_name<-page_html %>% html_nodes("h1")%>%html_text()
TITLE[m]<-journal_name[2]
name<-page_html %>% html_nodes("div.auths")%>%html_text()
PUBLISHER[m]<-name
publication_year<-page_html %>% html_nodes("div.cit")%>%html_text()
JOURN_NAME[m]<-sub("[.](.*)","",publication_year[1])
part1<-sub("(.*?)[.] ","",publication_year[1])
part2<-sub("[;](.*)","",part1)
YEAR[m]<-gsub("[a-zA-z ]","",part2)
PMID<-page_html %>% html_nodes("dd")%>%html_text()
CITATION[m]<-paste0(name,'"',journal_name[2],'"',publication_year,"PMID: ",PMID[2])
}
head_tables<-vector(mode="character", length=20)
a<-0
for (i in 1:length(head_lines)){
regex_head<-grepl("Table",head_lines[i],ignore.case=FALSE)
if (regex_head=="TRUE"){
a<-a+1
head_tables[a]<-head_lines[i]
}
}
head_tables<-head_tables[head_tables!=""]
head_tables_clean<-vector(mode="character", length=20)
c<-0
for (i in 1:length(head_tables)){
number<-length(unlist(str_extract_all(head_tables[i], "\\w+")))
if (number>2){
c<-c+1
head_tables_clean[c]<-head_tables[i]
}
}
head_tables_clean<-head_tables_clean[head_tables_clean!=""]
BOD_IN<-vector(mode="integer", length=20)
BOD_OUT<-vector(mode="integer", length=20)
BOD_REMOVAL<-vector(mode="integer", length=20)
sec_in<-0
sec_out<-0
sec_removal<-0
if(length(table)>0){
for(b in 1:length(table)){
con5<-0
con6<-0
con2<-0
con3<-0
con4<-0
regex_influent<-grepl("(?i)influent(?-i)",head_tables_clean[b],ignore.case=FALSE)|grepl("(?i)inflow(?-i)",head_tables_clean[b],ignore.case=FALSE)
if (regex_influent==TRUE){
con2<-1
}
regex_efluent<-grepl("(?i)efluent(?-i)",head_tables_clean[b],ignore.case=FALSE)|grepl("(?i)outflow(?-i)",head_tables_clean[b],ignore.case=FALSE)
if (regex_efluent==TRUE){
con3<-1
}
regex_removal<-grepl("(?i)removal(?i)",head_tables_clean[b],ignore.case=FALSE)
if (regex_removal==TRUE){
con4<-1
}
con<-0
row<-nrow(table[[b]])
col<-ncol(table[[b]])
for(i in 1:row){
for (j in 1:col){
table[[b]][i,j]<-sub("(?i)Â(.*)(?-i)","",table[[b]][i,j])
regex<-grepl("BOD",table[[b]][i,j],ignore.case=FALSE)|grepl("BOD",colnames(table[[b]])[j],ignore.case=FALSE)
if (regex=="TRUE"){
con<-con+1
}
}
}
table_numeric<-data.frame(table[[b]],stringsAsFactors=FALSE)
table_numeric<- as.data.frame(sapply(table_numeric, as.numeric))
elimination_colum<-vector(mode="integer", length=10)
elimination_row<-vector(mode="integer", length=10)
if (con>=1){
for (j in 1:col){
regex_deviation<-grepl("(?i)deviation(?-i)",table[[b]][1,j],ignore.case=FALSE)
regex_deviation_names<-grepl("(?i)deviation(?-i)",colnames(table[[b]][j]),ignore.case=FALSE)
if((regex_deviation==TRUE)|(regex_deviation_names==TRUE)){
con5<-con5+1
elimination_colum[con5]<-j
}
}
for (i in 1:row){
regex_deviation<-grepl("(?i)deviation(?-i)",table[[b]][i,1],ignore.case=FALSE)
if(regex_deviation==TRUE){
con6<-con6+1
elimination_row[con6]<-i
}
}
elimination_colum<-elimination_colum[elimination_colum!=0]
elimination_row<-elimination_row[elimination_row!=0]
if ((length(elimination_colum)!=0)&(length(elimination_row)!=0)){
table[[b]]<-table[[b]][-elimination_row,-elimination_colum]
}
if ((length(elimination_colum)==0)&(length(elimination_row)!=0)){
table[[b]]<-table[[b]][-elimination_row,]
}
if ((length(elimination_colum)!=0)&(length(elimination_row)==0)){
table[[b]]<-table[[b]][,-elimination_colum]
}
}
row<-nrow(table[[b]])
col<-ncol(table[[b]])
if ((con2>=1)&(con>=1)){
for (i in 1:2){
for (j in 1:col){
regex_in<-grepl("BOD",table[[b]][i,j],ignore.case=FALSE)
k<-i+1
if(k<=row){
if ((regex_in=="TRUE")&(is.numeric(table_numeric[k,j])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
p<-i+1
for (u in p:row){
sec_in<-sec_in+1
BOD_IN[sec_in]<-table[[b]][u,j]
}
}
}
}
}
if (BOD_IN[1]==0){
for (j in 1:col){
regex_in<-grepl("BOD",colnames(table[[b]])[j],ignore.case=FALSE)&grepl("(?i)in(?-i)",table[[b]][1,j],ignore.case=FALSE)
if (regex_in=="TRUE"){
for (i in 2:row){
sec_in<-sec_in+1
BOD_IN[sec_in]<-table[[b]][i,j]
}
}
}
}
for (i in 1:row) {
regex_in<-grepl("BOD",table[[b]][i,1],ignore.case=FALSE)
if ((regex_in=="TRUE")&(is.numeric(table_numeric[i,2])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
for (u in 2:col){
sec_in<-sec_in+1
BOD_IN[sec_in]<-table[[b]][i,u]
}
}
}
BOD_IN<-BOD_IN[BOD_IN!=0]
BOD_inlist[[m]]<-BOD_IN
}
if ((con3>=1)&(con>=1)){
for (i in 1:2){
for (j in 1:col){
regex_out<-grepl("BOD",table[[b]][i,j],ignore.case=FALSE)
k<-i+1
if(k<=row){
if ((regex_out=="TRUE")&(is.numeric(table_numeric[k,j])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
p<-i+1
for (u in p:row){
sec_out<-sec_out+1
BOD_OUT[sec_out]<-table[[b]][u,j]
}
}
}
}
}
if (BOD_OUT[1]==0){
for (j in 1:col){
regex_out<-grepl("BOD",colnames(table[[b]])[j],ignore.case=FALSE)&grepl("(?i)out(?-i)",table[[b]][1,j],ignore.case=FALSE)
if (regex_out=="TRUE"){
for (i in 2:row){
sec_out<-sec_out+1
BOD_OUT[sec_out]<-table[[b]][i,j]
}
}
}
}
for (i in 1:row) {
regex_out<-grepl("BOD",table[[b]][i,1],ignore.case=FALSE)
if ((regex_out=="TRUE")&(is.numeric(table_numeric[i,2])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
for (u in 2:col){
sec_out<-sec_out+1
BOD_OUT[sec_out]<-table[[b]][i,u]
}
}
}
BOD_OUT<-BOD_OUT[BOD_OUT!=0]
BOD_outlist[[m]]<-BOD_OUT
}
if ((con4>=1)&(con>=1)){
sec<-0
for (i in 1:2){
for (j in 1:col){
regex_removal<-grepl("BOD",table[[b]][i,j],ignore.case=FALSE)
k<-i+1
if(k<=row){
if ((regex_removal=="TRUE")&(is.numeric(table_numeric[k,j])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
p<-i+1
for (u in p:row){
sec_removal<-sec_removal+1
BOD_REMOVAL[sec_removal]<-table[[b]][u,j]
}
}
}
}
}
for (i in 1:row) {
regex_removal<-grepl("BOD",table[[b]][i,1],ignore.case=FALSE)
if ((regex_removal=="TRUE")&(is.numeric(table_numeric[i,2])==TRUE)&(is.na(table_numeric[k,j])==FALSE)){
for (u in 2:col){
sec_removal<-sec_removal+1
BOD_REMOVAL[sec_removal]<-table[[b]][i,u]
}
}
}
BOD_REMOVAL<-BOD_REMOVAL[BOD_REMOVAL!=0]
BOD_removallist[[m]]<-BOD_REMOVAL
}
}
}
}
}
}
for (i in 1:N.docs){
if (length(BOD_inlist[[i]])==0){
BOD_inlist[[i]]<-c(NA)
}
if (length(BOD_outlist[[i]])==0){
BOD_outlist[[i]]<-c(NA)
}
if (length(BOD_removallist[[i]])==0){
BOD_removallist[[i]]<-c(NA)
}
BOD_removallist[[i]]<-as.numeric(BOD_removallist[[i]])
BOD_outlist[[i]]<-as.numeric(BOD_outlist[[i]])
BOD_inlist[[i]]<-as.numeric(BOD_inlist[[i]])
}
YEAR
library(data.table)
warnings()
library("knitr")
knit2html("Wetlands_Aplication.R")
library("knitr")
knitr::opts_chunk$set(echo = TRUE)
library(NLP)
library(xml2)
library(proto)
library(RSQLite)
library(readr)
library(tm)
library(SnowballC)
library(RWeka)
library(sqldf)
library(dplyr)
library(data.table)
library(ggmap)
library(RODBC)
library(XML)
library(rvest)
library(xml2)
library(openxlsx)
library(RPostgreSQL)
library(stringr)
library(data.table)
library(stringr)
library(bitops)
library(RCurl)
library(rjson)
library(sp)
library(postGIStools)
rm(list=ls())
##PRE - PROCESSING OF DOCUMENTS
## Step 1: To convert the document from PDF to .txt by opening the PDF file in Microsoft WORD and then save it as .txt
## Step 2: To eliminate whitespaces and tap spaces by using the program Note++: (1) To enter tool "Search" (2) To enter tool "Replace" (3) To write the regular expression [\t\r\n\s]+ in the field: "Find what"
## Step 3: To name the documents in the form: doc[#].txt e.g. doc1.txt
## Step 4: To copy the processed files in the folder "C:/Users/xxx/Desktop/WETLANDS_EXTRATION/Documents"
## Step 5: To set the working directory with the location of the files - Folder: "C:/xxx/xxx/xxx/WETLANDS_EXTRATION/Documents"
setwd("C:/Users/Mauricio/Desktop/WETLANDS_EXTRATION/Documents")
##Extracted parameters:
##1- Parameter_1: Location - COUNTRY
##2- Parameter_2: Location - COUNTRY_CODE
##3- Parameter_3: Location - MUNICIPALITY
##4- Parameter_4: Location - LONGITUDE
##5- Parameter_5: Location - LATITUDE
##6- Parameter_6: TYPE OF WETLAND
##7- Parameter_7: TYPE OF WASTEWATER
##8- Parameter_8: TYPE OF PLANTS
##9- Parameter_9: BOD5 INFLUENT
##10- Parameter_10:BOD5 EFLUENT
##11- Parameter_11:JOURNAL NAME
##CLEANING AND CONSTRUCTION OF TEXT MATRIX
## Step 1: To read the files - Folder: "C:/Users/xxx/Desktop/WETLANDS_EXTRATION/Documents"
files_names<-list.files(getwd())
doc.list<-lapply(files_names,read_file)
N.docs<-length(doc.list)
## Step 2: To construct Corpus of documents
my.docs<-VectorSource(c(doc.list))
my.docs$names<-sub(".txt","",files_names)
mydocs<-Corpus(my.docs)
mydocs
mydocs[1]
mydocs[[1]]$content
